{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fcb129",
   "metadata": {
    "id": "71fcb129"
   },
   "source": [
    "# Module 13: Multiple Linear Regression and Polynomial Regression\n",
    "\n",
    "This notebook contains all coding parts for Module 13 using a real world dataset from `scikit-learn`.\n",
    "\n",
    "We will work through the following sections:\n",
    "1. Section 0: Setup and Data Loading\n",
    "2. Section 1: Introduction to Multiple Linear Regression (Notebook part)\n",
    "3. Section 3: Coding Multiple Linear Regression on Real Data\n",
    "4. Section 4: Introduction to Polynomial Regression (Notebook part)\n",
    "5. Section 6: Coding Polynomial Regression on Real Data\n",
    "\n",
    "Sections 2 and 5 are math and concept heavy, so those will be handled in slides, not in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y9fm5vnw0e",
   "metadata": {},
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "Machine Learning is a field of artificial intelligence where computers learn patterns from data without being explicitly programmed. Instead of writing rules to solve a problem, we show the computer examples and let it figure out the patterns itself.\n",
    "\n",
    "**Why is it used?**\n",
    "- To solve complex problems where writing explicit rules is difficult or impossible\n",
    "- To find hidden patterns in large datasets\n",
    "- To make predictions or decisions based on past experience\n",
    "\n",
    "**What problems does it solve?**\n",
    "- Predicting house prices based on features\n",
    "- Classifying emails as spam or not spam\n",
    "- Recommending products to customers\n",
    "- Recognizing images or speech\n",
    "\n",
    "**How does it work?**\n",
    "1. Collect data with known outcomes\n",
    "2. Split data into training and testing sets\n",
    "3. Train a model on the training data\n",
    "4. Test the model on unseen data\n",
    "5. Evaluate performance and improve\n",
    "\n",
    "Example: If we want to predict house prices, we collect data about houses (size, location, age) and their actual selling prices. The model learns how these features affect the price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e201e91b",
   "metadata": {
    "id": "e201e91b"
   },
   "source": [
    "## Section 0: Setup and Data Loading\n",
    "\n",
    "In this section we:\n",
    "- Import all required libraries\n",
    "- Load the California Housing dataset from `scikit-learn`\n",
    "- Wrap it into a pandas DataFrame\n",
    "- Inspect the basic structure and summary statistics\n",
    "\n",
    "The California Housing dataset is based on real census information, so it is a good example of a real world regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8l674wwnpzo",
   "metadata": {},
   "source": [
    "### What is a Dataset?\n",
    "\n",
    "A dataset is a collection of data organized in a structured format, typically as rows and columns (like a spreadsheet). In machine learning, datasets contain features (input variables) and targets (what we want to predict).\n",
    "\n",
    "**Why is it used?**\n",
    "- Provides the raw material for training machine learning models\n",
    "- Allows us to understand patterns and relationships in data\n",
    "- Helps us evaluate model performance on known examples\n",
    "\n",
    "**What problems does it solve?**\n",
    "- Without data, we cannot train or test machine learning models\n",
    "- Provides the ground truth for supervised learning\n",
    "- Enables validation of model predictions\n",
    "\n",
    "**How does it work?**\n",
    "- Each row represents an observation/example (e.g., one house)\n",
    "- Each column represents a feature or attribute (e.g., house size, number of rooms)\n",
    "- The target column contains what we want to predict (e.g., house price)\n",
    "- Quality and quantity of data directly impact model performance\n",
    "\n",
    "Example: In a housing dataset, each row is a house, columns include features like square footage, bedrooms, location, and the target is the selling price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "634eb2d6",
   "metadata": {
    "id": "634eb2d6"
   },
   "outputs": [],
   "source": [
    "# Section 0: Setup - Import libraries\n",
    "# These libraries are essential tools for machine learning in Python\n",
    "\n",
    "import numpy as np              # NumPy: For numerical operations and arrays\n",
    "                                # Provides efficient mathematical operations on large arrays\n",
    "                                # Used for numerical computations and data manipulation\n",
    "\n",
    "import pandas as pd             # Pandas: For working with tabular data (DataFrames)\n",
    "                                # Makes it easy to load, clean, and analyze structured data\n",
    "                                # Provides data structures and functions for data manipulation\n",
    "\n",
    "import matplotlib.pyplot as plt # Matplotlib: For plotting graphs and visualizations\n",
    "                                # Creates static, animated, and interactive visualizations\n",
    "                                # Essential for exploring data patterns and model results\n",
    "\n",
    "# Scikit-learn modules for machine learning tasks\n",
    "from sklearn.datasets import fetch_california_housing   # Real world regression dataset\n",
    "                                                              # Provides access to the California Housing dataset\n",
    "                                                              # Contains demographic and housing data for California districts\n",
    "\n",
    "from sklearn.model_selection import train_test_split    # Train test split utility\n",
    "                                                              # Splits data into training and testing sets\n",
    "                                                              # Essential for evaluating model performance on unseen data\n",
    "\n",
    "from sklearn.linear_model import LinearRegression       # Multiple Linear Regression model\n",
    "                                                              # Implements linear regression with multiple features\n",
    "                                                              # Finds the best linear relationship between features and target\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Evaluation metrics\n",
    "                                                              # mean_absolute_error: Average absolute difference between predictions and actual values\n",
    "                                                              # mean_squared_error: Average squared difference between predictions and actual values\n",
    "                                                              # r2_score: Proportion of variance in target explained by features (0-1, higher is better)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures    # For generating polynomial features\n",
    "                                                              # Creates polynomial combinations of input features\n",
    "                                                              # Enables modeling of non-linear relationships\n",
    "\n",
    "from sklearn.pipeline import Pipeline                   # For chaining preprocessing and model\n",
    "                                                              # Combines multiple steps into a single estimator\n",
    "                                                              # Simplifies workflows by automating sequences of operations\n",
    "\n",
    "# Configure matplotlib for slightly nicer default plots\n",
    "plt.rcParams['figure.figsize'] = (8, 5)  # Set default figure size to 8x5 inches\n",
    "plt.rcParams['axes.grid'] = True         # Show grid lines on plots for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a798d295",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1765035797515,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "a798d295",
    "outputId": "38a5d2c4-b739-45a4-b2de-c4ea74a77fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0368</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.761658</td>\n",
       "      <td>1.103627</td>\n",
       "      <td>413.0</td>\n",
       "      <td>2.139896</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.6591</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.931907</td>\n",
       "      <td>0.951362</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>2.128405</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.1200</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.797527</td>\n",
       "      <td>1.061824</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1.788253</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0804</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>2.026891</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>2.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.6912</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.970588</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>2.172269</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "5  4.0368      52.0  4.761658   1.103627       413.0  2.139896     37.85   \n",
       "6  3.6591      52.0  4.931907   0.951362      1094.0  2.128405     37.84   \n",
       "7  3.1200      52.0  4.797527   1.061824      1157.0  1.788253     37.84   \n",
       "8  2.0804      42.0  4.294118   1.117647      1206.0  2.026891     37.84   \n",
       "9  3.6912      52.0  4.970588   0.990196      1551.0  2.172269     37.84   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  \n",
       "5    -122.25        2.697  \n",
       "6    -122.25        2.992  \n",
       "7    -122.25        2.414  \n",
       "8    -122.26        2.267  \n",
       "9    -122.25        2.611  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the California Housing dataset\n",
    "# This dataset contains information about housing in California districts\n",
    "\n",
    "california = fetch_california_housing(as_frame=True)  # Load dataset as pandas DataFrame\n",
    "                                                      # as_frame=True returns data in a convenient DataFrame format\n",
    "                                                      # Contains features like median income, house age, average rooms, etc.\n",
    "\n",
    "# `california.frame` is a pandas DataFrame that already includes\n",
    "# both the features and the target column (MedHouseVal)\n",
    "\n",
    "df = california.frame.copy()  # Create a copy of the DataFrame\n",
    "                               # Using .copy() ensures we don't modify the original data\n",
    "                               # Good practice to work with a copy to preserve the original dataset\n",
    "\n",
    "print(df.shape)  # Print the shape: (number of rows, number of columns)\n",
    "                # This tells us we have 20,640 houses/districts and 9 columns\n",
    "df.head(10)      # Display the first 10 rows of the dataset\n",
    "                # .head() gives us a quick preview of the data structure and values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdWUbnTnXb0",
   "metadata": {
    "id": "0cdWUbnTnXb0"
   },
   "source": [
    "**Feature Descriptions:**\n",
    "\n",
    "1. **MedInc:** Median income of households in the block (measured in tens of thousands of US dollars).\n",
    "\n",
    "2. **HouseAge:** Median age of the houses in the block (in years).\n",
    "\n",
    "3. **AveRooms:** Average number of rooms per household in the block.\n",
    "\n",
    "4. **AveBedrms:** Average number of bedrooms per household in the block.\n",
    "\n",
    "5. **Population:** Total number of people living in the block.\n",
    "\n",
    "6. **AveOccup:** Average number of occupants per household in the block.\n",
    "\n",
    "7. **Latitude:** Geographic latitude of the block; higher values indicate farther north.\n",
    "\n",
    "8. **Longitude:** Geographic longitude of the block; higher values indicate farther west.\n",
    "\n",
    "9. **MedHouseVal:** Median house value for households in the block (in US dollars)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "so4k0yfc4en",
   "metadata": {},
   "source": [
    "### Understanding Features and Targets\n",
    "\n",
    "In machine learning:\n",
    "- **Features (X)**: Input variables we use to make predictions (also called predictors, independent variables)\n",
    "- **Target (y)**: What we want to predict (also called response, dependent variable, label)\n",
    "\n",
    "**Why is this distinction important?**\n",
    "- Features are the information we have (e.g., house size, location, age)\n",
    "- Target is what we want to predict (e.g., house price)\n",
    "- Models learn the relationship between features and target\n",
    "\n",
    "**What problems does this solve?**\n",
    "- Allows us to use known information to predict unknown values\n",
    "- Helps us understand which factors are most influential\n",
    "- Enables decision-making based on data patterns\n",
    "\n",
    "**How does it work?**\n",
    "- Features are fed into the model as input\n",
    "- The model learns patterns connecting features to target\n",
    "- For new data with only features, the model predicts the target\n",
    "- Quality of features directly impacts prediction accuracy\n",
    "\n",
    "Example: To predict house prices (target), we use features like square footage, number of bedrooms, location, and age of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9963bdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1765035797534,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "a9963bdb",
    "outputId": "272a156d-ed9b-4092-a93f-4ec8b2b23faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MedInc       20640 non-null  float64\n",
      " 1   HouseAge     20640 non-null  float64\n",
      " 2   AveRooms     20640 non-null  float64\n",
      " 3   AveBedrms    20640 non-null  float64\n",
      " 4   Population   20640 non-null  float64\n",
      " 5   AveOccup     20640 non-null  float64\n",
      " 6   Latitude     20640 non-null  float64\n",
      " 7   Longitude    20640 non-null  float64\n",
      " 8   MedHouseVal  20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Basic information about the dataset\n",
    "# .info() provides a summary of the DataFrame including:\n",
    "# - Number of entries (rows)\n",
    "# - Number of columns and their names\n",
    "# - Count of non-null values in each column\n",
    "# - Data type of each column (int64, float64, object, etc.)\n",
    "# - Memory usage of the DataFrame\n",
    "\n",
    "df.info()\n",
    "# This helps us understand:\n",
    "# 1. Whether we have missing data (null values)\n",
    "# 2. Data types of each column (important for model input)\n",
    "# 3. Overall size of our dataset\n",
    "# From the output, we see all columns have 20,640 non-null values (no missing data)\n",
    "# All columns are numeric (float64), which is perfect for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d0418b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1765035797696,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "88d0418b",
    "outputId": "42dfb31f-1afd-4aac-a3bc-68ed120ca0a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>3.870671</td>\n",
       "      <td>1.899822</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>2.563400</td>\n",
       "      <td>3.534800</td>\n",
       "      <td>4.743250</td>\n",
       "      <td>15.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>5.429000</td>\n",
       "      <td>2.474173</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>4.440716</td>\n",
       "      <td>5.229129</td>\n",
       "      <td>6.052381</td>\n",
       "      <td>141.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>1.096675</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.006079</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1.099526</td>\n",
       "      <td>34.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>3.070655</td>\n",
       "      <td>10.386050</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>2.429741</td>\n",
       "      <td>2.818116</td>\n",
       "      <td>3.282261</td>\n",
       "      <td>1243.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>41.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>-119.569704</td>\n",
       "      <td>2.003532</td>\n",
       "      <td>-124.350000</td>\n",
       "      <td>-121.800000</td>\n",
       "      <td>-118.490000</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>-114.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedHouseVal</th>\n",
       "      <td>20640.0</td>\n",
       "      <td>2.068558</td>\n",
       "      <td>1.153956</td>\n",
       "      <td>0.149990</td>\n",
       "      <td>1.196000</td>\n",
       "      <td>1.797000</td>\n",
       "      <td>2.647250</td>\n",
       "      <td>5.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count         mean          std         min         25%  \\\n",
       "MedInc       20640.0     3.870671     1.899822    0.499900    2.563400   \n",
       "HouseAge     20640.0    28.639486    12.585558    1.000000   18.000000   \n",
       "AveRooms     20640.0     5.429000     2.474173    0.846154    4.440716   \n",
       "AveBedrms    20640.0     1.096675     0.473911    0.333333    1.006079   \n",
       "Population   20640.0  1425.476744  1132.462122    3.000000  787.000000   \n",
       "AveOccup     20640.0     3.070655    10.386050    0.692308    2.429741   \n",
       "Latitude     20640.0    35.631861     2.135952   32.540000   33.930000   \n",
       "Longitude    20640.0  -119.569704     2.003532 -124.350000 -121.800000   \n",
       "MedHouseVal  20640.0     2.068558     1.153956    0.149990    1.196000   \n",
       "\n",
       "                     50%          75%           max  \n",
       "MedInc          3.534800     4.743250     15.000100  \n",
       "HouseAge       29.000000    37.000000     52.000000  \n",
       "AveRooms        5.229129     6.052381    141.909091  \n",
       "AveBedrms       1.048780     1.099526     34.066667  \n",
       "Population   1166.000000  1725.000000  35682.000000  \n",
       "AveOccup        2.818116     3.282261   1243.333333  \n",
       "Latitude       34.260000    37.710000     41.950000  \n",
       "Longitude    -118.490000  -118.010000   -114.310000  \n",
       "MedHouseVal     1.797000     2.647250      5.000010  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics for numeric columns\n",
    "# .describe() provides key statistics for each numeric column:\n",
    "# - count: Number of non-null values\n",
    "# - mean: Average value\n",
    "# - std: Standard deviation (measure of spread)\n",
    "# - min: Minimum value\n",
    "# - 25%: First quartile (25th percentile)\n",
    "# - 50%: Median (50th percentile)\n",
    "# - 75%: Third quartile (75th percentile)\n",
    "# - max: Maximum value\n",
    "\n",
    "df.describe().T  # .T transposes the result for better readability\n",
    "                 # This rotates the table so features are rows and statistics are columns\n",
    "\n",
    "# Key insights from these statistics:\n",
    "# 1. MedInc (Median Income): Ranges from 0.5 to 15, mean of 3.87\n",
    "# 2. HouseAge: From 1 to 52 years, average 29 years\n",
    "# 3. MedHouseVal (Target): From $149,990 to $500,001, average $206,856\n",
    "# 4. Some features have very different scales (e.g., Population vs AveBedrms)\n",
    "#    - This suggests we might need feature scaling for some models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc99360d",
   "metadata": {
    "id": "bc99360d"
   },
   "source": [
    "## Section 1: Introduction to Multiple Linear Regression (Notebook part)\n",
    "\n",
    "In multiple linear regression, we use several input features together to predict a single numeric target.\n",
    "\n",
    "In this dataset:\n",
    "- **Target (y)**: `MedHouseVal` (median house value in a block)\n",
    "- **Candidate features (X)**: income, house age, average rooms, average bedrooms, population, average occupancy\n",
    "\n",
    "In this section we will:\n",
    "- Select a subset of features as our input matrix `X`\n",
    "- Set `MedHouseVal` as the target `y`\n",
    "- Inspect the shapes\n",
    "- Look at simple correlations to build intuition about relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9koqd18v5j",
   "metadata": {},
   "source": [
    "### Understanding Correlation\n",
    "\n",
    "Correlation measures how strongly two variables are related to each other. The correlation coefficient ranges from -1 to 1:\n",
    "- 1: Perfect positive correlation (when one increases, the other always increases)\n",
    "- 0: No correlation (variables move independently)\n",
    "- -1: Perfect negative correlation (when one increases, the other always decreases)\n",
    "\n",
    "**Why is it important?**\n",
    "- Helps identify which features are most related to the target\n",
    "- Reveals multicollinearity (high correlation between features)\n",
    "- Guides feature selection and model interpretation\n",
    "\n",
    "**What problems does it solve?**\n",
    "- Identifying redundant features (high correlation with each other)\n",
    "- Finding the most promising predictors\n",
    "- Understanding relationships in the data\n",
    "\n",
    "**How to interpret correlation values:**\n",
    "- 0.7 to 1.0: Strong positive correlation\n",
    "- 0.3 to 0.7: Moderate positive correlation\n",
    "- -0.3 to 0.3: Weak or no correlation\n",
    "- -0.7 to -0.3: Moderate negative correlation\n",
    "- -1.0 to -0.7: Strong negative correlation\n",
    "\n",
    "**Best practices:**\n",
    "- Always visualize correlations with plots\n",
    "- Consider removing one of two highly correlated features\n",
    "- Remember: correlation doesn't imply causation!\n",
    "- Use domain knowledge to interpret correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nxjwsjepqer",
   "metadata": {},
   "source": [
    "### What is Multiple Linear Regression?\n",
    "\n",
    "Multiple Linear Regression (MLR) is a statistical technique that uses multiple input features to predict a single numeric target variable. It extends simple linear regression (which uses one feature) by incorporating two or more features.\n",
    "\n",
    "**Why is it used?**\n",
    "- To model real-world relationships where outcomes depend on multiple factors\n",
    "- To understand how each feature contributes to the target\n",
    "- To make predictions based on several input variables\n",
    "\n",
    "**What problems does it solve?**\n",
    "- Predicting house prices based on size, location, age, etc.\n",
    "- Estimating sales based on advertising budget, season, and competitor prices\n",
    "- Forecasting crop yields based on rainfall, temperature, and fertilizer use\n",
    "\n",
    "**How does it work?**\n",
    "The model learns a linear equation:\n",
    "```\n",
    "y = b0 + b1*x1 + b2*x2 + ... + bn*xn\n",
    "```\n",
    "Where:\n",
    "- y is the target variable\n",
    "- x1, x2, ..., xn are the input features\n",
    "- b0 is the intercept (baseline value when all features are 0)\n",
    "- b1, b2, ..., bn are coefficients (how much each feature affects the target)\n",
    "\n",
    "**Example**: Predicting a house price might use:\n",
    "- x1 = square footage\n",
    "- x2 = number of bedrooms\n",
    "- x3 = age of house\n",
    "- x4 = median income in neighborhood\n",
    "\n",
    "The model learns how much each factor contributes to the final price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cb5ea3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1765035797800,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "60cb5ea3",
    "outputId": "3219273e-1e28-44a0-ed3e-b26d35d0b802"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0368</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.761658</td>\n",
       "      <td>1.103627</td>\n",
       "      <td>413.0</td>\n",
       "      <td>2.139896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.6591</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.931907</td>\n",
       "      <td>0.951362</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>2.128405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.1200</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.797527</td>\n",
       "      <td>1.061824</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1.788253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0804</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>2.026891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.6912</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.970588</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>2.172269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556\n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842\n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260\n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945\n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467\n",
       "5  4.0368      52.0  4.761658   1.103627       413.0  2.139896\n",
       "6  3.6591      52.0  4.931907   0.951362      1094.0  2.128405\n",
       "7  3.1200      52.0  4.797527   1.061824      1157.0  1.788253\n",
       "8  2.0804      42.0  4.294118   1.117647      1206.0  2.026891\n",
       "9  3.6912      52.0  4.970588   0.990196      1551.0  2.172269"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define target and feature columns for our Multiple Linear Regression model\n",
    "\n",
    "target_col = 'MedHouseVal'  # This is what we want to predict (median house value)\n",
    "                            # In ML terminology, this is our 'y' variable or 'target'\n",
    "\n",
    "# Select features we'll use to predict the target\n",
    "feature_cols = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n",
    "# These features are chosen based on domain knowledge:\n",
    "# - MedInc: Higher income areas typically have more expensive houses\n",
    "# - HouseAge: Age can affect house value (newer vs. historic)\n",
    "# - AveRooms: More rooms usually means larger houses\n",
    "# - AveBedrms: Bedroom count affects house value\n",
    "# - Population: Density might affect prices\n",
    "# - AveOccup: Occupancy rate can indicate neighborhood quality\n",
    "\n",
    "X = df[feature_cols]  # Create feature matrix X (uppercase by convention)\n",
    "                      # This is a DataFrame containing all our input features\n",
    "                      # Shape will be (20640, 6) - 20640 rows, 6 columns\n",
    "\n",
    "y = df[target_col]    # Create target vector y (lowercase by convention)\n",
    "                      # This is a Series containing our target variable\n",
    "                      # Shape will be (20640,) - 20640 rows, 1 column\n",
    "\n",
    "X.head(10)  # Display first 10 rows of our feature matrix\n",
    "           # This helps verify we selected the correct features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3ae405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1765035797959,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "5b3ae405",
    "outputId": "364ef5f1-c653-4c98-f09b-feaa6b393735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (20640, 6)\n",
      "Shape of y:  (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes of X and y to understand our data structure\n",
    "\n",
    "print('Shape of X: ', X.shape)  # Shape will be (number_of_rows, number_of_features)\n",
    "                                # Output: (20640, 6) means:\n",
    "                                # - 20,640 houses/districts in our dataset\n",
    "                                # - 6 features selected for each house\n",
    "                                # This 2D structure is required for scikit-learn models\n",
    "\n",
    "print('Shape of y: ', y.shape)  # Shape will be (number_of_rows,) for a 1D array\n",
    "                                # Output: (20640,) means:\n",
    "                                # - 20,640 target values (one for each house)\n",
    "                                # - The comma indicates it's a 1D array, not a scalar\n",
    "                                # This is the expected format for scikit-learn targets\n",
    "\n",
    "# Understanding these shapes is crucial because:\n",
    "# 1. Most scikit-learn models expect X to be 2D (samples Ã— features)\n",
    "# 2. The target y should be 1D with the same number of samples as X\n",
    "# 3. Mismatched shapes will cause errors when fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13db02e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1765035798912,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "13db02e8",
    "outputId": "e4b08bba-bc1a-4433-a5fd-b8d56c675adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix: \n",
      "               MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
      "MedInc       1.000000 -0.119034  0.326895  -0.062040    0.004834  0.018766   \n",
      "HouseAge    -0.119034  1.000000 -0.153277  -0.077747   -0.296244  0.013191   \n",
      "AveRooms     0.326895 -0.153277  1.000000   0.847621   -0.072213 -0.004852   \n",
      "AveBedrms   -0.062040 -0.077747  0.847621   1.000000   -0.066197 -0.006181   \n",
      "Population   0.004834 -0.296244 -0.072213  -0.066197    1.000000  0.069863   \n",
      "AveOccup     0.018766  0.013191 -0.004852  -0.006181    0.069863  1.000000   \n",
      "MedHouseVal  0.688075  0.105623  0.151948  -0.046701   -0.024650 -0.023737   \n",
      "\n",
      "             MedHouseVal  \n",
      "MedInc          0.688075  \n",
      "HouseAge        0.105623  \n",
      "AveRooms        0.151948  \n",
      "AveBedrms      -0.046701  \n",
      "Population     -0.024650  \n",
      "AveOccup       -0.023737  \n",
      "MedHouseVal     1.000000  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple correlation matrix between selected features and the target\n",
    "corr_matrix = df[feature_cols + [target_col]].corr()\n",
    "print(\"Correlation matrix: \")\n",
    "print(corr_matrix)\n",
    "\n",
    "\n",
    "# Plot correlation matrix using matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.imshow(corr_matrix.values, interpolation='nearest')\n",
    "ax.set_xticks(range(len(corr_matrix.columns)))\n",
    "ax.set_yticks(range(len(corr_matrix.index)))\n",
    "ax.set_xticklabels(corr_matrix.columns, rotation=45, ha='right')\n",
    "ax.set_yticklabels(corr_matrix.index)\n",
    "fig.colorbar(cax)\n",
    "ax.set_title('Correlation matrix for selected features and target')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35eaa4",
   "metadata": {
    "id": "6e35eaa4"
   },
   "source": [
    "## Section 3: Coding Multiple Linear Regression on Real Data\n",
    "\n",
    "In this section we implement a full workflow for multiple linear regression:\n",
    "\n",
    "1. Split the data into training and test sets\n",
    "2. Fit a `LinearRegression` model on the training data\n",
    "3. Inspect the learned coefficients and intercept\n",
    "4. Make predictions on train and test sets\n",
    "5. Evaluate the model using MAE, RMSE, and R squared\n",
    "6. Visualize predicted vs actual values\n",
    "7. Plot residuals to check basic patterns\n",
    "\n",
    "We will use the feature matrix `X` and target `y` defined in Section 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s8wyumprmxn",
   "metadata": {},
   "source": [
    "### Understanding Model Coefficients and Intercept\n",
    "\n",
    "In Multiple Linear Regression, the model learns:\n",
    "- **Intercept (bâ‚€)**: The predicted value when all features are 0\n",
    "- **Coefficients (bâ‚, bâ‚‚, ..., bâ‚™)**: How much the target changes for a one-unit increase in each feature\n",
    "\n",
    "**Interpreting coefficients:**\n",
    "- Positive coefficient: As the feature increases, the target tends to increase\n",
    "- Negative coefficient: As the feature increases, the target tends to decrease\n",
    "- Larger magnitude: Stronger effect on the target\n",
    "- Near zero: Little effect on the target\n",
    "\n",
    "**Example interpretation:**\n",
    "If MedInc has coefficient 0.5:\n",
    "- For every $10,000 increase in median income (MedInc unit)\n",
    "- House value increases by $50,000 (0.5 Ã— $100,000)\n",
    "- Holding all other features constant\n",
    "\n",
    "**Important considerations:**\n",
    "- Coefficients assume linear relationships\n",
    "- They can be affected by feature scaling\n",
    "- Correlated features can make coefficients unstable\n",
    "- Always consider domain knowledge when interpreting\n",
    "\n",
    "**Common mistakes:**\n",
    "- Confusing correlation with causation\n",
    "- Ignoring the scale of features\n",
    "- Not considering feature interactions\n",
    "- Over-interpreting small coefficient differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fmlbbmiaed8",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics\n",
    "\n",
    "When building regression models, we need metrics to evaluate how well our predictions match the actual values. Common metrics include:\n",
    "\n",
    "**1. Mean Absolute Error (MAE)**\n",
    "- Average of absolute differences between predicted and actual values\n",
    "- Easy to interpret (same units as target)\n",
    "- Less sensitive to outliers than MSE\n",
    "\n",
    "**2. Mean Squared Error (MSE)**\n",
    "- Average of squared differences between predicted and actual values\n",
    "- Penalizes large errors more heavily (squares amplify them)\n",
    "- Not in original units (squared units)\n",
    "\n",
    "**3. Root Mean Squared Error (RMSE)**\n",
    "- Square root of MSE\n",
    "- In same units as target (unlike MSE)\n",
    "- Most commonly used regression metric\n",
    "\n",
    "**4. R-squared (RÂ²)**\n",
    "- Proportion of variance in target explained by features\n",
    "- Ranges from 0 to 1 (or negative for very poor models)\n",
    "- 0.70 means model explains 70% of variance in target\n",
    "- Higher is better, but can be misleading on non-linear data\n",
    "\n",
    "**Interpreting metrics:**\n",
    "- Lower MAE/RMSE = better accuracy\n",
    "- Higher RÂ² = better fit (but watch out for overfitting)\n",
    "- Compare training vs test metrics to detect overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv1h4o7gkxo",
   "metadata": {},
   "source": [
    "### What is Train-Test Split?\n",
    "\n",
    "Train-test split is a fundamental technique in machine learning where we divide our dataset into:\n",
    "- **Training set**: Used to train/fit the model (typically 70-80% of data)\n",
    "- **Test set**: Used to evaluate model performance on unseen data (typically 20-30% of data)\n",
    "\n",
    "**Why is it used?**\n",
    "- To get an unbiased estimate of model performance\n",
    "- To detect overfitting (when model memorizes training data but can't generalize)\n",
    "- To ensure our model works on new, unseen data\n",
    "\n",
    "**What problems does it solve?**\n",
    "- Prevents \"cheating\" by evaluating on data the model has already seen\n",
    "- Provides a realistic estimate of real-world performance\n",
    "- Helps choose between different models or hyperparameters\n",
    "\n",
    "**How does it work?**\n",
    "1. Randomly shuffle the dataset (with random_state for reproducibility)\n",
    "2. Split into training and test sets based on test_size (e.g., 0.2 = 20%)\n",
    "3. Train model only on training set\n",
    "4. Evaluate on test set (which the model has never seen)\n",
    "\n",
    "**Best practices:**\n",
    "- Always set random_state for reproducible results\n",
    "- Never look at test data during model development\n",
    "- For small datasets, consider cross-validation\n",
    "- Ensure both sets represent the overall data distribution\n",
    "\n",
    "Example: If we have 1000 houses, we might use 800 for training and 200 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a87461",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1765035798917,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "24a87461",
    "outputId": "df788d23-69b4-41e7-f707-4bfc40ec0913"
   },
   "outputs": [],
   "source": [
    "# Step 1: Train test split\n",
    "# We split our data to evaluate how well our model generalizes to unseen data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# Parameters explained:\n",
    "# X, y: Our features and target\n",
    "# test_size=0.2: Use 20% of data for testing, 80% for training\n",
    "# random_state=42: Ensures same split every time (reproducible results)\n",
    "#                 42 is just a convention - any number works\n",
    "\n",
    "print(\"Training set size: \", X_train.shape[0], \"rows\")  # Should be 80% of 20640 = 16512\n",
    "print(\"Test set size: \", X_test.shape[0], \"rows\")       # Should be 20% of 20640 = 4128\n",
    "\n",
    "# Why this split matters:\n",
    "# 1. Training set: Model learns patterns from this data\n",
    "# 2. Test set: Evaluates how well model learned (unseen data)\n",
    "# 3. Prevents overfitting detection\n",
    "# 4. Gives realistic performance estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f030fa5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1765035798927,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "2f030fa5",
    "outputId": "c254f034-0e62-4520-bc7b-549dbe3f9da1"
   },
   "outputs": [],
   "source": [
    "# Step 2: Create and fit the Linear Regression model\n",
    "\n",
    "lin_reg = LinearRegression()  # Create an instance of the LinearRegression class\n",
    "                              # This initializes an empty model with no learned parameters yet\n",
    "                              # Default parameters are fine for most cases\n",
    "\n",
    "# .fit() is the training method - this is where learning happens!\n",
    "lin_reg.fit(X_train, y_train)\n",
    "# What happens during .fit():\n",
    "# 1. Model calculates the best coefficients using Ordinary Least Squares (OLS)\n",
    "# 2. OLS finds coefficients that minimize sum of squared residuals\n",
    "# 3. Residual = actual value - predicted value\n",
    "# 4. Model learns the relationship: y = b0 + b1*x1 + b2*x2 + ... + bn*xn\n",
    "\n",
    "print('Model fitted successfully')  # Confirmation message\n",
    "# After fitting, the model has learned:\n",
    "# - lin_reg.coef_: The coefficients for each feature (b1, b2, ..., bn)\n",
    "# - lin_reg.intercept_: The intercept term (b0)\n",
    "# These parameters define our linear equation for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bd357",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1765035798934,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "d95bd357",
    "outputId": "85317e86-6726-468d-b0d8-daab90e8fdee"
   },
   "outputs": [],
   "source": [
    "# Step 3: Inspect learned parameters (coefficients and intercept)\n",
    "# These values define our linear regression equation\n",
    "\n",
    "print('Intercept (bias term): ', lin_reg.intercept_)\n",
    "# The intercept is the predicted house value when all features are 0\n",
    "# Negative intercept (-0.55) doesn't mean negative house prices - \n",
    "# it's just a mathematical artifact; features are never actually 0\n",
    "\n",
    "print('\\nCoefficients: ')\n",
    "for feature_name, coef in zip(feature_cols, lin_reg.coef_):\n",
    "  print(f'{feature_name}: {coef}')\n",
    "# Coefficients show how much house value changes with a one-unit increase in each feature\n",
    "# Positive = price increases, Negative = price decreases\n",
    "\n",
    "# Interpretation of key coefficients:\n",
    "# MedInc: 0.55 â†’ Every $10,000 increase in median income \n",
    "#                   increases house value by $55,000\n",
    "# HouseAge: 0.017 â†’ Each additional year adds $1,700 to value\n",
    "# AveRooms: -0.22 â†’ More rooms per house actually correlates with LOWER prices\n",
    "#                (This might be because many small rooms = smaller houses)\n",
    "# AveBedrms: 1.12 â†’ More bedrooms significantly increases price\n",
    "# Population: 0.00002 â†’ Effect is negligible\n",
    "# AveOccup: -0.005 â†’ Higher occupancy per house correlates with lower prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e73e86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1765035798966,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "53e73e86",
    "outputId": "b1a057f3-e18e-45df-9dea-25442848a646"
   },
   "outputs": [],
   "source": [
    "# Step 4: Make predictions on training and test sets\n",
    "# .predict() applies the learned linear equation to new data\n",
    "\n",
    "y_train_pred = lin_reg.predict(X_train)  # Predictions on training data\n",
    "                                         # Model has seen this data during training\n",
    "                                         # Good for checking if model learned the training patterns\n",
    "\n",
    "y_test_pred = lin_reg.predict(X_test)    # Predictions on test data\n",
    "                                         # Model has NEVER seen this data\n",
    "                                         # This is the true test of model performance\n",
    "\n",
    "print('Some sample predictions on test set (first 5 rows): ')\n",
    "print('Predicted: ', y_test_pred[:5])    # Our model's predictions\n",
    "                                          # Values are in same units as target (house values)\n",
    "print('Actual:    ', y_test.values[:5])  # True house values\n",
    "                                          # Comparing these helps assess accuracy\n",
    "\n",
    "# How predictions work:\n",
    "# For each row in X_test, model calculates:\n",
    "# prediction = intercept + (coef1 * feature1) + (coef2 * feature2) + ... \n",
    "# Example: If a house has MedInc=5, HouseAge=20, etc.,\n",
    "# prediction = -0.55 + (0.55 * 5) + (0.017 * 20) + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebaefa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1765035799007,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "aebaefa4",
    "outputId": "addb60e5-595f-4b10-e7e4-fe128ddf126a"
   },
   "outputs": [],
   "source": [
    "# Step 5: Define a helper function to print evaluation metrics\n",
    "# This function will help us compare model performance consistently\n",
    "\n",
    "def regression_metrics(y_true, y_pred, label='Model'):\n",
    "  \"\"\"\n",
    "  Calculate and display common regression metrics.\n",
    "  \n",
    "  Parameters:\n",
    "  y_true: Actual target values (ground truth)\n",
    "  y_pred: Predicted values from model\n",
    "  label: Name to identify the model being evaluated\n",
    "  \"\"\"\n",
    "  mae = mean_absolute_error(y_true, y_pred)\n",
    "  # MAE: Mean Absolute Error - average absolute difference\n",
    "  # Easy to interpret (same units as target)\n",
    "  \n",
    "  mse = mean_squared_error(y_true, y_pred)\n",
    "  # MSE: Mean Squared Error - average of squared differences\n",
    "  # Penalizes large errors more heavily\n",
    "  \n",
    "  rmse = np.sqrt(mse)\n",
    "  # RMSE: Root Mean Squared Error - square root of MSE\n",
    "  # Most common metric, in same units as target\n",
    "  \n",
    "  r2 = r2_score(y_true, y_pred)\n",
    "  # RÂ²: R-squared - proportion of variance explained\n",
    "  # 0.50 means model explains 50% of the variation\n",
    "\n",
    "  print(f'=== {label} ====')\n",
    "  print('MAE: ', mae)  # Lower is better\n",
    "  print('RMSE: ', rmse)  # Lower is better\n",
    "  print('R2 : ', r2)   # Higher is better (max 1.0)\n",
    "  print()\n",
    "\n",
    "# Evaluate on train and test sets\n",
    "regression_metrics(y_train, y_train_pred, label='Linear Regression (Train)')\n",
    "# Training metrics show how well model learned the training data\n",
    "# High RÂ² and low errors suggest good learning\n",
    "\n",
    "regression_metrics(y_test, y_test_pred, label='Linear Regression (Test)')\n",
    "# Test metrics show how well model generalizes to new data\n",
    "# Compare train vs test to detect overfitting:\n",
    "# - If train RÂ² >> test RÂ²: Overfitting\n",
    "# - If similar: Good generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f069d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1765035799867,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "039f069d",
    "outputId": "2ed16e30-41f3-4a04-9fdc-81ff28d75552"
   },
   "outputs": [],
   "source": [
    "# Step 6: Plot predicted vs actual values on the test set\n",
    "# This visualization helps us understand model performance qualitatively\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.3)\n",
    "# Scatter plot of actual (x-axis) vs predicted (y-axis) house values\n",
    "# alpha=0.3 makes points semi-transparent to see density\n",
    "# Each point represents one house in the test set\n",
    "\n",
    "plt.xlabel('Actual MedHouseVal')  # True house values\n",
    "plt.ylabel('Predicted MedHouseVal')  # Model's predictions\n",
    "plt.title('Actual vs Predicted House Values (Test set)')\n",
    "\n",
    "# Diagonal reference line (perfect prediction line)\n",
    "min_val = min(y_test.min(), y_test_pred.min())\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], linestyle='--')\n",
    "# This dashed line represents perfect predictions (predicted = actual)\n",
    "# Points closer to this line indicate better predictions\n",
    "# Points above line: model overpredicts\n",
    "# Points below line: model underpredicts\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What this plot tells us:\n",
    "# 1. Pattern: Points should follow a line from bottom-left to top-right\n",
    "# 2. Spread: Wider spread = less accurate predictions\n",
    "# 3. Bias: If points consistently above/below line, model is biased\n",
    "# 4. Range: Check if model handles all value ranges equally well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984fbba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1765035800761,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "2984fbba",
    "outputId": "46ca643f-ec74-4304-8af1-10eb01b9711e"
   },
   "outputs": [],
   "source": [
    "# Step 7: Residual plot (errors = actual - predicted)\n",
    "# Residuals help diagnose model problems and assumptions\n",
    "\n",
    "residuals = y_test - y_test_pred\n",
    "# Residual = actual value - predicted value\n",
    "# Positive residual: model underpredicted (actual > predicted)\n",
    "# Negative residual: model overpredicted (actual < predicted)\n",
    "# Residual near zero: accurate prediction\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.3)\n",
    "# Plot residuals against predicted values\n",
    "# This helps identify patterns in prediction errors\n",
    "\n",
    "plt.axhline(0, linestyle='--')\n",
    "# Horizontal line at residual = 0\n",
    "# Represents perfect predictions (no error)\n",
    "# Points above: underpredictions\n",
    "# Points below: overpredictions\n",
    "\n",
    "plt.xlabel('Predicted MedHouseVal')\n",
    "plt.ylabel('Residual (Actual - Predicted)')\n",
    "plt.title('Residual plot (Test set)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What to look for in residual plots:\n",
    "# 1. Random scatter around zero line: Good (model assumptions met)\n",
    "# 2. Clear pattern (U-shape, curve): Model missing non-linear relationships\n",
    "# 3. Funnel shape (wider spread at ends): Heteroscedasticity (error variance changes)\n",
    "# 4. Outliers: Points far from zero might be data quality issues\n",
    "# 5. Clusters: Different error patterns for different value ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba35e6",
   "metadata": {
    "id": "b7ba35e6"
   },
   "source": [
    "## Section 4: Introduction to Polynomial Regression (Notebook part)\n",
    "\n",
    "Polynomial regression keeps the model **linear in parameters** but allows nonlinear relationships between the input feature and the target by adding polynomial terms.\n",
    "\n",
    "In this notebook part we:\n",
    "- Use a **single feature** `MedInc` (median income) to predict `MedHouseVal`\n",
    "- Fit a simple **linear model** on this single feature\n",
    "- Visualize the linear fit\n",
    "\n",
    "Later, in Section 6, we will extend this to polynomial regression using `PolynomialFeatures` and `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9xwrrt719w",
   "metadata": {},
   "source": [
    "### Understanding Overfitting and Underfitting\n",
    "\n",
    "These are fundamental concepts in machine learning that describe how well a model captures the underlying patterns in data.\n",
    "\n",
    "**Underfitting** (High Bias, Low Variance)\n",
    "- Model is too simple to capture the true pattern\n",
    "- Poor performance on both training and test data\n",
    "- Symptoms: Low RÂ² on training set, systematic patterns in residuals\n",
    "- Solution: Increase model complexity (e.g., higher degree polynomial, more features)\n",
    "\n",
    "**Good Fit** (Balanced Bias-Variance)\n",
    "- Model captures true patterns without memorizing noise\n",
    "- Good performance on both training and test data\n",
    "- Similar performance on train and test sets\n",
    "- Random scatter in residuals\n",
    "\n",
    "**Overfitting** (Low Bias, High Variance)\n",
    "- Model is too complex, memorizes training data including noise\n",
    "- Excellent performance on training data, poor on test data\n",
    "- Symptoms: High training RÂ², much lower test RÂ²\n",
    "- Solution: Reduce complexity, add regularization, get more data\n",
    "\n",
    "**The Bias-Variance Tradeoff**\n",
    "- Increasing model complexity reduces bias but increases variance\n",
    "- Decreasing complexity reduces variance but increases bias\n",
    "- Goal: Find sweet spot with minimum total error\n",
    "- Cross-validation helps find optimal complexity\n",
    "\n",
    "**Visual intuition:**\n",
    "- Underfit: Straight line trying to fit a curve\n",
    "- Good fit: Smooth curve following the trend\n",
    "- Overfit: Wiggly curve passing through every point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ln3o5s4rcjp",
   "metadata": {},
   "source": [
    "### What is Polynomial Regression?\n",
    "\n",
    "Polynomial Regression is a form of linear regression where the relationship between the independent variable X and dependent variable y is modeled as an nth degree polynomial. Despite the \"polynomial\" name, it's still considered \"linear\" because the model is linear in its coefficients.\n",
    "\n",
    "**Why is it used?**\n",
    "- To capture non-linear relationships in data\n",
    "- When linear regression underfits (high bias)\n",
    "- When residual plots show curved patterns\n",
    "- To model more complex real-world relationships\n",
    "\n",
    "**What problems does it solve?**\n",
    "- Housing prices that don't increase linearly with features\n",
    "- Diminishing returns (e.g., each additional bedroom adds less value)\n",
    "- Optimal points (e.g., crop yield peaks at certain rainfall)\n",
    "- U-shaped or S-shaped relationships\n",
    "\n",
    "**How does it work?**\n",
    "Instead of: y = bâ‚€ + bâ‚x\n",
    "We use: y = bâ‚€ + bâ‚x + bâ‚‚xÂ² + bâ‚ƒxÂ³ + ... + bâ‚™xâ¿\n",
    "\n",
    "The model learns coefficients for each polynomial term, allowing curves instead of straight lines.\n",
    "\n",
    "**Important concepts:**\n",
    "- **Degree 1**: Simple linear regression (straight line)\n",
    "- **Degree 2**: Quadratic (parabola) - can model one curve\n",
    "- **Degree 3**: Cubic - can model S-shapes\n",
    "- **Higher degrees**: More complex curves (risk of overfitting)\n",
    "\n",
    "**Example**: House value vs income might start steep, then level off as income increases (diminishing returns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa78378",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1765035963066,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "8aa78378",
    "outputId": "382dda91-9c16-48a5-f80d-a8236de2cb4a"
   },
   "outputs": [],
   "source": [
    "# Prepare a single feature for illustration: MedInc vs MedHouseVal\n",
    "# We'll use just one feature to visualize polynomial regression clearly\n",
    "\n",
    "X_single = df[['MedInc']]  # DataFrame with one column (note double brackets!)\n",
    "                          # Double brackets ensure we get a DataFrame (2D)\n",
    "                          # Single bracket would give a Series (1D)\n",
    "                          # scikit-learn expects 2D input for features\n",
    "\n",
    "y_single = df[target_col]  # Same target as before (median house value)\n",
    "\n",
    "# Split the single feature data (same random_state for fair comparison)\n",
    "X_single_train, X_single_test, y_single_train, y_single_test = train_test_split(\n",
    "    X_single, y_single, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print('Single feature training shape:', X_single_train.shape)\n",
    "# Shape will be (16512, 1) - 16512 samples, 1 feature\n",
    "print('Single feature test shape    :', X_single_test.shape)\n",
    "# Shape will be (4128, 1) - 4128 samples, 1 feature\n",
    "\n",
    "# Using one feature allows us to:\n",
    "# 1. Visualize the relationship clearly with scatter plots\n",
    "# 2. Show how polynomial regression creates curves\n",
    "# 3. Demonstrate overfitting with high-degree polynomials\n",
    "# 4. Compare linear vs polynomial fits visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3d7ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1765036115280,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "4fe3d7ec",
    "outputId": "0059b42a-bc72-4710-9d01-2c36c16c4176"
   },
   "outputs": [],
   "source": [
    "# Fit a simple Linear Regression model using only MedInc\n",
    "# This is our baseline - degree 1 polynomial (straight line)\n",
    "\n",
    "lin_reg_single = LinearRegression()\n",
    "# Create a new linear regression model for single feature\n",
    "lin_reg_single.fit(X_single_train, y_single_train)\n",
    "# Train the model using only median income as predictor\n",
    "\n",
    "y_single_test_pred = lin_reg_single.predict(X_single_test)\n",
    "# Make predictions on the test set\n",
    "\n",
    "regression_metrics(y_single_test, y_single_test_pred, label='Single Feature Linear Regression (Test)')\n",
    "# Evaluate performance\n",
    "\n",
    "# Interpretation of results:\n",
    "# RÂ² of ~0.46 means:\n",
    "# - Using only MedInc explains 46% of variation in house prices\n",
    "# - Not bad for a single feature, but room for improvement\n",
    "# - The remaining 54% is explained by other factors not in model\n",
    "\n",
    "# Why single feature model is useful:\n",
    "# 1. Baseline for comparison with polynomial models\n",
    "# 2. Shows how much one good feature can explain\n",
    "# 3. Easier to visualize and understand\n",
    "# 4. Faster to train and predict\n",
    "\n",
    "# Expected relationship:\n",
    "# Higher income â†’ higher house prices\n",
    "# But might not be perfectly linear (diminishing returns at high incomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82036b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1765036174627,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "4c82036b",
    "outputId": "6eb8f2b6-2630-49ae-9029-6aec4fd59fdf"
   },
   "outputs": [],
   "source": [
    "# Visualize the linear fit for the single feature model\n",
    "# This plot helps us see how well a straight line captures the relationship\n",
    "\n",
    "# Create a grid of MedInc values for a smooth line\n",
    "X_plot = np.linspace(X_single['MedInc'].min(), X_single['MedInc'].max(), 200).reshape(-1, 1)\n",
    "# linspace creates 200 evenly spaced values between min and max MedInc\n",
    "# reshape(-1, 1) converts to 2D array (required by scikit-learn)\n",
    "# This gives us points to draw a smooth prediction line\n",
    "\n",
    "y_plot_lin = lin_reg_single.predict(X_plot)\n",
    "# Get predictions for our grid points\n",
    "# This creates the straight line showing model's predictions\n",
    "\n",
    "plt.figure()\n",
    "# Scatter plot of actual data points\n",
    "plt.scatter(X_single_train['MedInc'], y_single_train, alpha=0.2, label='Train data')\n",
    "# alpha=0.2 makes points transparent to see density\n",
    "# We use training data to see what the model learned from\n",
    "\n",
    "# Plot the linear regression line\n",
    "plt.plot(X_plot, y_plot_lin, linewidth=2, label='Linear fit (degree 1)')\n",
    "# linewidth=2 makes the line more visible\n",
    "# This shows the model's prediction for any MedInc value\n",
    "\n",
    "plt.xlabel('MedInc')  # x-axis: Median income (in tens of thousands)\n",
    "plt.ylabel('MedHouseVal')  # y-axis: Median house value (in hundreds of thousands)\n",
    "plt.title('Single feature linear regression: MedInc vs MedHouseVal')\n",
    "plt.legend()  # Show the labels we defined\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What this visualization shows:\n",
    "# 1. Data distribution: How house values vary with income\n",
    "# 2. Linear fit: The straight line the model learned\n",
    "# 3. Model limitations: Where the line doesn't capture the pattern well\n",
    "# 4. Potential for improvement: Curved patterns that polynomial could capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae355f4e",
   "metadata": {
    "id": "ae355f4e"
   },
   "source": [
    "## Section 6: Coding Polynomial Regression on Real Data\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Use the same single feature `MedInc`\n",
    "- Create polynomial features of different degrees (1, 2, 3, and 5)\n",
    "- Fit a linear regression model on these expanded features using `Pipeline`\n",
    "- Compare performance (R squared and RMSE) for each degree\n",
    "\n",
    "Key new tools:\n",
    "- `PolynomialFeatures`: expands original features into polynomial combinations\n",
    "- `Pipeline`: chains polynomial expansion and linear regression into one convenient object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mf05swiim6d",
   "metadata": {},
   "source": [
    "### What is a Pipeline?\n",
    "\n",
    "A Pipeline is a scikit-learn tool that chains multiple processing steps together into a single estimator. It sequentially applies a list of transforms and a final estimator.\n",
    "\n",
    "**Why is it used?**\n",
    "- To prevent data leakage between train and test sets\n",
    "- To simplify the workflow by combining steps\n",
    "- To ensure consistent preprocessing\n",
    "- To make code cleaner and more maintainable\n",
    "\n",
    "**What problems does it solve?**\n",
    "- Forgetting to apply the same transformations to test data\n",
    "- Accidentally fitting preprocessing on test data\n",
    "- Complex and repetitive code\n",
    "- Order dependency of operations\n",
    "\n",
    "**How does it work?**\n",
    "```python\n",
    "Pipeline([\n",
    "    ('step1_name', transformer1),\n",
    "    ('step2_name', transformer2),\n",
    "    ('model', estimator)\n",
    "])\n",
    "```\n",
    "When you call `.fit()` on pipeline:\n",
    "1. transformer1.fit_transform() is called\n",
    "2. transformer2.fit_transform() is called on result\n",
    "3. estimator.fit() is called on final transformed data\n",
    "\n",
    "**Benefits:**\n",
    "- `.fit()` and `.predict()` handle all steps automatically\n",
    "- Prevents common preprocessing mistakes\n",
    "- Easy to save and load the entire workflow\n",
    "- Grid search works on all parameters\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('lin_reg', LinearRegression())\n",
    "])\n",
    "```\n",
    "This will create polynomial features, then fit linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7sz676hh86d",
   "metadata": {},
   "source": [
    "### What is PolynomialFeatures?\n",
    "\n",
    "PolynomialFeatures is a scikit-learn transformer that creates polynomial combinations of input features. It transforms the feature matrix to include polynomial terms of a specified degree.\n",
    "\n",
    "**Why is it used?**\n",
    "- To generate polynomial terms without manual calculation\n",
    "- To enable linear regression to fit curves\n",
    "- To capture non-linear relationships in data\n",
    "\n",
    "**What problems does it solve?**\n",
    "- Tedious manual creation of polynomial terms\n",
    "- Consistent transformation of train and test data\n",
    "- Handling multiple features automatically\n",
    "- Including interaction terms between features\n",
    "\n",
    "**How does it work?**\n",
    "For a single feature X and degree=2:\n",
    "- Input: [xâ‚, xâ‚‚, xâ‚ƒ]\n",
    "- Output: [xâ‚, xâ‚Â², xâ‚‚, xâ‚‚Â², xâ‚ƒ, xâ‚ƒÂ²]\n",
    "\n",
    "For two features [Xâ‚, Xâ‚‚] and degree=2:\n",
    "- Input: [xâ‚â‚, xâ‚â‚‚], [xâ‚‚â‚, xâ‚‚â‚‚]\n",
    "- Output: [1, xâ‚â‚, xâ‚â‚‚, xâ‚â‚Â², xâ‚â‚xâ‚â‚‚, xâ‚â‚‚Â²]\n",
    "\n",
    "**Parameters:**\n",
    "- `degree`: Highest polynomial degree to include\n",
    "- `include_bias`: Whether to add column of 1s (intercept term)\n",
    "- `interaction_only`: Only include interaction terms, not powers\n",
    "\n",
    "**Example with X=[2,3], degree=2, include_bias=False:**\n",
    "Output: [2, 3, 4, 6, 9]\n",
    "- Original: 2, 3\n",
    "- Squared: 4 (2Â²), 9 (3Â²)\n",
    "- Interaction: 6 (2Ã—3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e56b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1765041308136,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "f61e56b4",
    "outputId": "ba878c79-9450-49f5-c9fd-7ead2570d2b5"
   },
   "outputs": [],
   "source": [
    "# Compare polynomial regression models of different degrees on the single feature MedInc\n",
    "# We'll test degrees 1, 2, 3, and 5 to see how complexity affects performance\n",
    "\n",
    "degrees = [1,2,3,5]  # Different polynomial degrees to test\n",
    "results = []         # Store results for comparison\n",
    "\n",
    "for deg in degrees:\n",
    "  # Create a pipeline that:\n",
    "  # 1. Transforms features into polynomial terms\n",
    "  # 2. Fits linear regression on the transformed features\n",
    "  model = Pipeline([\n",
    "      ('poly', PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "      # include_bias=False because LinearRegression has its own intercept\n",
    "      # degree=deg determines highest polynomial term\n",
    "      # For degree=2: creates x and xÂ² terms\n",
    "      # For degree=3: creates x, xÂ², and xÂ³ terms\n",
    "      \n",
    "      ('lin_reg', LinearRegression())\n",
    "      # Linear regression on the polynomial features\n",
    "      # Note: Still \"linear\" because coefficients are linear\n",
    "      # The relationship between x and y can be non-linear\n",
    "  ])\n",
    "\n",
    "  # Fit the pipeline on training data\n",
    "  model.fit(X_single_train, y_single_train)\n",
    "  # This automatically:\n",
    "  # 1. Creates polynomial features from X_single_train\n",
    "  # 2. Fits linear regression on these features\n",
    "\n",
    "  # Make predictions on both train and test sets\n",
    "  y_train_pred_deg = model.predict(X_single_train)\n",
    "  y_test_pred_deg = model.predict(X_single_test)\n",
    "  # Pipeline automatically applies polynomial transformation before predicting\n",
    "\n",
    "  # Calculate metrics for evaluation\n",
    "  mae_train = mean_absolute_error(y_single_train, y_train_pred_deg)\n",
    "  rmse_train = np.sqrt(mean_squared_error(y_single_train, y_train_pred_deg))\n",
    "  r2_train = r2_score(y_single_train, y_train_pred_deg)\n",
    "\n",
    "  mae_test = mean_absolute_error(y_single_test, y_test_pred_deg)\n",
    "  rmse_test = np.sqrt(mean_squared_error(y_single_test, y_test_pred_deg))\n",
    "  r2_test = r2_score(y_single_test, y_test_pred_deg)\n",
    "\n",
    "  # Store results for comparison\n",
    "  results.append({\n",
    "      'degree': deg,\n",
    "      'MAE_train': mae_train,\n",
    "      'RMSE_train': rmse_train,\n",
    "      'R2_train': r2_train,\n",
    "      'MAE_test': mae_test,\n",
    "      'RMSE_test': rmse_test,\n",
    "      'R2_test': r2_test,\n",
    "  })\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n",
    "# This table shows how model complexity (degree) affects performance\n",
    "# Look for:\n",
    "# 1. Increasing RÂ² with degree (improvement)\n",
    "# 2. Gap between train and test performance (overfitting signs)\n",
    "# 3. Best test performance (optimal degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee42ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1765041311952,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "adee42ee",
    "outputId": "aa95b118-8ae7-4f48-c496-85ec6d91e55f"
   },
   "outputs": [],
   "source": [
    "# Plot R squared and RMSE vs polynomial degree\n",
    "# This visualization helps us understand the bias-variance tradeoff\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,7))\n",
    "# Create a figure with 2 subplots side by side\n",
    "# figsize makes it wide enough for both plots\n",
    "\n",
    "# R squared plot (left subplot)\n",
    "axes[0].plot(results_df['degree'], results_df['R2_train'], marker='o', label='Train R2')\n",
    "# Plot training RÂ² vs degree with circle markers\n",
    "axes[0].plot(results_df['degree'], results_df['R2_test'], marker='o', label='Test R2')\n",
    "# Plot test RÂ² vs degree with circle markers\n",
    "\n",
    "axes[0].set_xlabel(\"Polynomial degree\")\n",
    "axes[0].set_ylabel(\"R2 Score\")\n",
    "axes[0].set_title(\"R2 Score vs polynomial degree (for single feature)\")\n",
    "axes[0].legend()\n",
    "# Customize the left plot\n",
    "\n",
    "# RMSE plot (right subplot)\n",
    "axes[1].plot(results_df['degree'], results_df['RMSE_train'], marker='o', label='Train RMSE')\n",
    "# Plot training RMSE vs degree\n",
    "axes[1].plot(results_df['degree'], results_df['RMSE_test'], marker='o', label='Test RMSE')\n",
    "# Plot test RMSE vs degree\n",
    "\n",
    "axes[1].set_xlabel(\"Polynomial degree\")\n",
    "axes[1].set_ylabel(\"RMSE Score\")\n",
    "axes[1].set_title(\"RMSE Score vs polynomial degree (for single feature)\")\n",
    "axes[1].legend()\n",
    "# Customize the right plot\n",
    "\n",
    "plt.tight_layout()\n",
    "# Adjust spacing to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "# What these plots show:\n",
    "# \n",
    "# RÂ² plot:\n",
    "# - Higher RÂ² = better model\n",
    "# - Train RÂ² always increases with degree (model gets more flexible)\n",
    "# - Test RÂ² shows real performance\n",
    "# - Gap between train and test indicates overfitting\n",
    "#\n",
    "# RMSE plot:\n",
    "# - Lower RMSE = better model\n",
    "# - Train RMSE always decreases with degree\n",
    "# - Test RMSE shows where model generalizes best\n",
    "# - Look for the \"elbow\" where test RMSE starts increasing\n",
    "\n",
    "# Expected pattern:\n",
    "# - Low degree: Both scores low (underfitting)\n",
    "# - Optimal degree: Test scores peak/bottom\n",
    "# - High degree: Train high, test low (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851307b7",
   "metadata": {
    "id": "851307b7"
   },
   "source": [
    "### Optional: Polynomial Regression with Two Features\n",
    "\n",
    "The main ideas of polynomial regression can extend to more than one feature. This example demonstrates using two features together.\n",
    "\n",
    "**When to use multiple features with polynomial regression:**\n",
    "- When multiple factors interact to affect the outcome\n",
    "- When the relationship between features isn't additive\n",
    "- When feature combinations matter (e.g., income AND location together)\n",
    "\n",
    "**What happens with multiple features:**\n",
    "For features Xâ‚ and Xâ‚‚ with degree=2:\n",
    "- Original terms: Xâ‚, Xâ‚‚\n",
    "- Squared terms: Xâ‚Â², Xâ‚‚Â²\n",
    "- Interaction term: Xâ‚ Ã— Xâ‚‚\n",
    "\n",
    "**Interaction terms** capture how features work together:\n",
    "- Example: High income might matter more in good locations\n",
    "- The effect of income might depend on the house age\n",
    "- These are captured by the Xâ‚Ã—Xâ‚‚ term\n",
    "\n",
    "**Caution with multiple features:**\n",
    "- Number of features grows rapidly with degree\n",
    "- Degree 2 with 5 features creates 20 features!\n",
    "- Risk of overfitting increases\n",
    "- Feature scaling becomes important\n",
    "\n",
    "**Best practices:**\n",
    "1. Start with low degrees (2 or 3)\n",
    "2. Consider feature selection first\n",
    "3. Use regularization for many features\n",
    "4. Always validate with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b044a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1765041363016,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "c14b044a",
    "outputId": "79ca0536-7702-46f2-bfd2-9f4d927daa01"
   },
   "outputs": [],
   "source": [
    "# Optional block: polynomial regression with two features (degree 2)\n",
    "# This example uses MedInc and HouseAge together to show feature interaction\n",
    "\n",
    "feature_cols_two = ['MedInc', 'HouseAge']\n",
    "# Select two features that might interact:\n",
    "# - MedInc: Higher income areas might value newer houses differently\n",
    "# - HouseAge: The effect of house age on price might depend on neighborhood income\n",
    "\n",
    "X_two = df[feature_cols_two]\n",
    "# Create feature matrix with two columns\n",
    "\n",
    "X_two_train, X_two_test, y_two_train, y_two_test = train_test_split(\n",
    "    X_two, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# Split into train and test sets\n",
    "# Same random_state ensures comparable splits\n",
    "\n",
    "deg = 2  # Use degree 2 for polynomial features\n",
    "# With 2 features and degree 2, we get:\n",
    "# - Original: MedInc, HouseAge\n",
    "# - Squared: MedIncÂ², HouseAgeÂ²\n",
    "# - Interaction: MedInc Ã— HouseAge\n",
    "# Total: 5 features instead of 2\n",
    "\n",
    "poly_model_two = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=deg, include_bias=False)),\n",
    "    # Transform 2 features into 5 polynomial features\n",
    "    ('lin_reg', LinearRegression())\n",
    "    # Fit linear regression on all polynomial terms\n",
    "])\n",
    "\n",
    "poly_model_two.fit(X_two_train, y_two_train)\n",
    "# Fit the pipeline:\n",
    "# 1. Creates polynomial features from training data\n",
    "# 2. Learns coefficients for each term\n",
    "\n",
    "y_two_test_pred = poly_model_two.predict(X_two_test)\n",
    "# Make predictions on test data\n",
    "\n",
    "# Evaluate performance\n",
    "regression_metrics(y_two_test, y_two_test_pred,\n",
    "                   label='Polynomial regression (degree 2, two features)')\n",
    "# Compare to single feature results:\n",
    "# - Adding a second feature should improve RÂ²\n",
    "# - Polynomial features capture non-linear relationships\n",
    "# - Interaction term captures how features work together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SHjHkA0LX5tY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1765041434915,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     },
     "user_tz": -360
    },
    "id": "SHjHkA0LX5tY",
    "outputId": "5e0d9241-7685-44d6-fb21-ace84815c3c9"
   },
   "outputs": [],
   "source": [
    "# All features with polynomial regression (degree 2)\n",
    "# This uses ALL available features to predict house values\n",
    "\n",
    "X_all = df.drop('MedHouseVal', axis=1)\n",
    "# Drop the target column to get all features\n",
    "# axis=1 means we're dropping a column\n",
    "# This includes latitude and longitude as well\n",
    "\n",
    "y = df['MedHouseVal']\n",
    "# Target variable remains the same\n",
    "\n",
    "# Train-test split (same as before)\n",
    "X_all_train, X_all_test, y_all_train, y_all_test = train_test_split(\n",
    "    X_all, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Polynomial Regression (degree 2) with all features\n",
    "degree = 2\n",
    "# With 8 features and degree 2:\n",
    "# - Original features: 8\n",
    "# - Squared terms: 8\n",
    "# - Interaction terms: 28 (8 choose 2)\n",
    "# Total: 44 features!\n",
    "\n",
    "poly_all_model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "    # This will create 44 features from 8 original features\n",
    "    ('lin_reg', LinearRegression())\n",
    "    # Fit linear regression on all polynomial features\n",
    "])\n",
    "\n",
    "poly_all_model.fit(X_all_train, y_all_train)\n",
    "# Fit the model on training data\n",
    "\n",
    "y_all_test_pred = poly_all_model.predict(X_all_test)\n",
    "# Make predictions on test set\n",
    "\n",
    "# Evaluate performance\n",
    "regression_metrics(y_all_test, y_all_test_pred,\n",
    "                   label='Polynomial Regression (degree 2, ALL features)')\n",
    "\n",
    "# Performance comparison:\n",
    "# - Single feature linear: RÂ² ~0.46\n",
    "# - Multiple feature linear: RÂ² ~0.51\n",
    "# - Single feature polynomial: RÂ² ~0.47\n",
    "# - All features polynomial: RÂ² ~0.65\n",
    "# \n",
    "# Key takeaways:\n",
    "# 1. More features generally improve performance\n",
    "# 2. Polynomial features help capture non-linear relationships\n",
    "# 3. Interaction terms between features add predictive power\n",
    "# 4. Beware of overfitting with many features and high degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gf7c67ets6w",
   "metadata": {},
   "source": [
    "### Summary of Key Concepts\n",
    "\n",
    "**1. Multiple Linear Regression**\n",
    "- Uses multiple features to predict a target\n",
    "- Learns linear equation: y = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ... + bâ‚™xâ‚™\n",
    "- Coefficients show feature importance\n",
    "- Assumes linear relationships\n",
    "\n",
    "**2. Polynomial Regression**\n",
    "- Captures non-linear relationships\n",
    "- Still linear in parameters (coefficients)\n",
    "- Uses PolynomialFeatures to create powers of features\n",
    "- Risk of overfitting with high degrees\n",
    "\n",
    "**3. Model Evaluation**\n",
    "- MAE: Average absolute error (interpretable)\n",
    "- RMSE: Square root of average squared error (most common)\n",
    "- RÂ²: Proportion of variance explained (0-1, higher better)\n",
    "- Always compare train vs test performance\n",
    "\n",
    "**4. Train-Test Split**\n",
    "- Prevents overfitting detection\n",
    "- Provides unbiased performance estimate\n",
    "- Typical split: 80% train, 20% test\n",
    "- Use random_state for reproducibility\n",
    "\n",
    "**5. Best Practices**\n",
    "- Start simple, add complexity if needed\n",
    "- Visualize relationships and residuals\n",
    "- Check for overfitting (train vs test gap)\n",
    "- Consider feature scaling for different units\n",
    "- Use pipelines to prevent data leakage"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10HT_tEGUvYOIyoi3mZZyj26mpbtg4Xaw",
     "timestamp": 1765185567149
    },
    {
     "file_id": "1xip0eNSzlQnDZGduba_WvHfA1v_etSGM",
     "timestamp": 1764947550108
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
