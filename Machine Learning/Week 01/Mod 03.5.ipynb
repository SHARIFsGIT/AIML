{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics:\n",
    "1. Standardization, Min-Max scaling, Robust scaling\n",
    "2. Nominal vs ordinal variables, one-hot vs ordinal encoding\n",
    "3. Vectors, dot product, norms, Euclidean and Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Quick Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. Spot the right scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1. Spot the right scaler\n",
      "\n",
      "a) Apartment_price_BDT with a few luxury penthouses\n",
      "   Answer: Robust Scaler\n",
      "   Justification: Handles outliers (luxury penthouses) well by using median and IQR\n",
      "\n",
      "b) Skin_temperature_C measured from a wearable between 30 and 36\n",
      "   Answer: Min-Max Scaler\n",
      "   Justification: Data is bounded with no outliers, perfect for Min-Max scaling to [0,1]\n",
      "\n",
      "c) Daily_app_opens with many zeros and a few power users\n",
      "   Answer: Robust Scaler\n",
      "   Justification: Power users create outliers, Robust scaler prevents them from dominating\n"
     ]
    }
   ],
   "source": [
    "print(\"A1. Spot the right scaler\\n\")\n",
    "print(\"a) Apartment_price_BDT with a few luxury penthouses\")\n",
    "print(\"   Answer: Robust Scaler\")\n",
    "print(\"   Justification: Handles outliers (luxury penthouses) well by using median and IQR\\n\")\n",
    "\n",
    "print(\"b) Skin_temperature_C measured from a wearable between 30 and 36\")\n",
    "print(\"   Answer: Min-Max Scaler\")\n",
    "print(\"   Justification: Data is bounded with no outliers, perfect for Min-Max scaling to [0,1]\\n\")\n",
    "\n",
    "print(\"c) Daily_app_opens with many zeros and a few power users\")\n",
    "print(\"   Answer: Robust Scaler\")\n",
    "print(\"   Justification: Power users create outliers, Robust scaler prevents them from dominating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Manual Min-Max on a tiny set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2. Manual Min-Max Scaling\n",
      "\n",
      "Original scores: [20, 25, 30, 50]\n",
      "\n",
      "Step 1: min = 20, max = 50\n",
      "\n",
      "Step 2: Apply formula (x - min) / (max - min)\n",
      "  (20 - 20) / (50 - 20) = 0 / 30 = 0.0\n",
      "  (25 - 20) / (50 - 20) = 5 / 30 = 0.16666666666666666\n",
      "  (30 - 20) / (50 - 20) = 10 / 30 = 0.3333333333333333\n",
      "  (50 - 20) / (50 - 20) = 30 / 30 = 1.0\n",
      "\n",
      "Scaled scores: [0.0, 0.16666666666666666, 0.3333333333333333, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"A2. Manual Min-Max Scaling\\n\")\n",
    "scores = [20, 25, 30, 50]\n",
    "print(f\"Original scores: {scores}\")\n",
    "\n",
    "# Step 1: Find min and max\n",
    "min_val = min(scores)\n",
    "max_val = max(scores)\n",
    "print(f\"\\nStep 1: min = {min_val}, max = {max_val}\")\n",
    "\n",
    "# Step 2: Apply formula (x - min) / (max - min)\n",
    "print(\"\\nStep 2: Apply formula (x - min) / (max - min)\")\n",
    "scaled_scores = []\n",
    "for score in scores:\n",
    "    scaled = (score - min_val) / (max_val - min_val)\n",
    "    scaled_scores.append(scaled)\n",
    "    print(f\"  ({score} - {min_val}) / ({max_val} - {min_val}) = {score - min_val} / {max_val - min_val} = {scaled}\")\n",
    "\n",
    "print(f\"\\nScaled scores: {scaled_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Z-scores on a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3. Z-scores (Standardization)\n",
      "\n",
      "Original values: [8, 9, 11]\n",
      "\n",
      "Step 1: Mean = 28 / 3 = 9.333333333333334\n",
      "\n",
      "Step 2: Variance = 1.5556\n",
      "        Std Dev = sqrt(1.5556) = 1.2472\n",
      "\n",
      "Step 3: Standardize using (x - mean) / std\n",
      "  (8 - 9.333333333333334) / 1.2472 = -1.0690\n",
      "  (9 - 9.333333333333334) / 1.2472 = -0.2673\n",
      "  (11 - 9.333333333333334) / 1.2472 = 1.3363\n",
      "\n",
      "Z-scores: [-1.069, -0.2673, 1.3363]\n"
     ]
    }
   ],
   "source": [
    "print(\"A3. Z-scores (Standardization)\\n\")\n",
    "x = [8, 9, 11]\n",
    "print(f\"Original values: {x}\")\n",
    "\n",
    "# Step 1: Calculate mean\n",
    "mean = sum(x) / len(x)\n",
    "print(f\"\\nStep 1: Mean = {sum(x)} / {len(x)} = {mean}\")\n",
    "\n",
    "# Step 2: Calculate population standard deviation\n",
    "variance = sum((xi - mean)**2 for xi in x) / len(x)\n",
    "std = variance ** 0.5\n",
    "print(f\"\\nStep 2: Variance = {variance:.4f}\")\n",
    "print(f\"        Std Dev = sqrt({variance:.4f}) = {std:.4f}\")\n",
    "\n",
    "# Step 3: Standardize each value\n",
    "print(\"\\nStep 3: Standardize using (x - mean) / std\")\n",
    "z_scores = []\n",
    "for val in x:\n",
    "    z = (val - mean) / std\n",
    "    z_scores.append(z)\n",
    "    print(f\"  ({val} - {mean}) / {std:.4f} = {z:.4f}\")\n",
    "\n",
    "print(f\"\\nZ-scores: {[round(z, 4) for z in z_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4. Robust scaling ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A4. Robust Scaling Ingredients\n",
      "\n",
      "Values: [5, 6, 6, 7, 50]\n",
      "\n",
      "Median: 6\n",
      "Q1 (25th percentile): 6.0\n",
      "Q3 (75th percentile): 7.0\n",
      "IQR (Q3 - Q1): 7.0 - 6.0 = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"A4. Robust Scaling Ingredients\\n\")\n",
    "y = [5, 6, 6, 7, 50]\n",
    "print(f\"Values: {y}\")\n",
    "\n",
    "# Median\n",
    "sorted_y = sorted(y)\n",
    "median = sorted_y[len(sorted_y) // 2]\n",
    "print(f\"\\nMedian: {median}\")\n",
    "\n",
    "# Q1 (25th percentile)\n",
    "Q1 = np.percentile(y, 25)\n",
    "print(f\"Q1 (25th percentile): {Q1}\")\n",
    "\n",
    "# Q3 (75th percentile)\n",
    "Q3 = np.percentile(y, 75)\n",
    "print(f\"Q3 (75th percentile): {Q3}\")\n",
    "\n",
    "# IQR\n",
    "IQR = Q3 - Q1\n",
    "print(f\"IQR (Q3 - Q1): {Q3} - {Q1} = {IQR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A5. Nominal or ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A5. Nominal or Ordinal\n",
      "\n",
      "a) T-shirt_size {S, M, L, XL}\n",
      "   Answer: ORDINAL (has natural ordering from small to extra large)\n",
      "\n",
      "b) City {Dhaka, Chattogram, Rajshahi}\n",
      "   Answer: NOMINAL (no inherent ordering between cities)\n",
      "\n",
      "c) Satisfaction {Low, Medium, High}\n",
      "   Answer: ORDINAL (has natural ordering from low to high)\n"
     ]
    }
   ],
   "source": [
    "print(\"A5. Nominal or Ordinal\\n\")\n",
    "print(\"a) T-shirt_size {S, M, L, XL}\")\n",
    "print(\"   Answer: ORDINAL (has natural ordering from small to extra large)\\n\")\n",
    "\n",
    "print(\"b) City {Dhaka, Chattogram, Rajshahi}\")\n",
    "print(\"   Answer: NOMINAL (no inherent ordering between cities)\\n\")\n",
    "\n",
    "print(\"c) Satisfaction {Low, Medium, High}\")\n",
    "print(\"   Answer: ORDINAL (has natural ordering from low to high)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Hands on Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1. Three scalers side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1. Three Scalers Side by Side\n",
      "\n",
      "Heights: [150, 160, 170, 175, 180]\n",
      "Weights: [58, 62, 65, 66, 190]\n",
      "\n",
      "a) Min-Max Scaling to [0, 1]\n",
      "Heights scaled: [0.0, 0.3333, 0.6667, 0.8333, 1.0]\n",
      "Weights scaled: [0.0, 0.0303, 0.053, 0.0606, 1.0]\n",
      "\n",
      "b) Standardize first three values\n",
      "Heights first 3 standardized: [np.float64(-1.2247), np.float64(0.0), np.float64(1.2247)]\n",
      "Weights first 3 standardized: [np.float64(-1.2787), np.float64(0.1162), np.float64(1.1625)]\n",
      "\n",
      "c) Robust Scaling for Weights\n",
      "Median: 65.0, Q1: 62.0, Q3: 66.0, IQR: 4.0\n",
      "Weights robust scaled: [np.float64(-1.75), np.float64(-0.75), np.float64(0.0), np.float64(0.25), np.float64(31.25)]\n",
      "\n",
      "d) Which scaler handles the outlier (190) best?\n",
      "   Answer: ROBUST SCALER\n",
      "   Min-Max scaled outlier: 1.0000 (becomes 1.0, dominates the scale)\n",
      "   Robust scaled outlier: 31.2500 (less extreme, better contained)\n"
     ]
    }
   ],
   "source": [
    "print(\"B1. Three Scalers Side by Side\\n\")\n",
    "Heights = [150, 160, 170, 175, 180]\n",
    "Weights = [58, 62, 65, 66, 190]\n",
    "\n",
    "print(f\"Heights: {Heights}\")\n",
    "print(f\"Weights: {Weights}\\n\")\n",
    "\n",
    "# a) Min-Max scale both to [0, 1]\n",
    "print(\"a) Min-Max Scaling to [0, 1]\")\n",
    "heights_min, heights_max = min(Heights), max(Heights)\n",
    "weights_min, weights_max = min(Weights), max(Weights)\n",
    "\n",
    "heights_minmax = [(h - heights_min) / (heights_max - heights_min) for h in Heights]\n",
    "weights_minmax = [(w - weights_min) / (weights_max - weights_min) for w in Weights]\n",
    "\n",
    "print(f\"Heights scaled: {[round(h, 4) for h in heights_minmax]}\")\n",
    "print(f\"Weights scaled: {[round(w, 4) for w in weights_minmax]}\\n\")\n",
    "\n",
    "# b) Standardize the first three values only\n",
    "print(\"b) Standardize first three values\")\n",
    "heights_first3 = Heights[:3]\n",
    "weights_first3 = Weights[:3]\n",
    "\n",
    "h_mean = np.mean(heights_first3)\n",
    "h_std = np.std(heights_first3, ddof=0)\n",
    "w_mean = np.mean(weights_first3)\n",
    "w_std = np.std(weights_first3, ddof=0)\n",
    "\n",
    "heights_std = [(h - h_mean) / h_std for h in heights_first3]\n",
    "weights_std = [(w - w_mean) / w_std for w in weights_first3]\n",
    "\n",
    "print(f\"Heights first 3 standardized: {[round(h, 4) for h in heights_std]}\")\n",
    "print(f\"Weights first 3 standardized: {[round(w, 4) for w in weights_std]}\\n\")\n",
    "\n",
    "# c) Robust scale Weights\n",
    "print(\"c) Robust Scaling for Weights\")\n",
    "w_median = np.median(Weights)\n",
    "w_q1 = np.percentile(Weights, 25)\n",
    "w_q3 = np.percentile(Weights, 75)\n",
    "w_iqr = w_q3 - w_q1\n",
    "\n",
    "print(f\"Median: {w_median}, Q1: {w_q1}, Q3: {w_q3}, IQR: {w_iqr}\")\n",
    "weights_robust = [(w - w_median) / w_iqr for w in Weights]\n",
    "print(f\"Weights robust scaled: {[round(w, 4) for w in weights_robust]}\\n\")\n",
    "\n",
    "# d) Which scaler handles outlier best\n",
    "print(\"d) Which scaler handles the outlier (190) best?\")\n",
    "print(\"   Answer: ROBUST SCALER\")\n",
    "print(f\"   Min-Max scaled outlier: {weights_minmax[-1]:.4f} (becomes 1.0, dominates the scale)\")\n",
    "print(f\"   Robust scaled outlier: {weights_robust[-1]:.4f} (less extreme, better contained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. One-hot by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B2. One-Hot Encoding by Hand\n",
      "\n",
      "Original Cities: ['Dhaka', 'Chattogram', 'Dhaka', 'Rajshahi', 'Rajshahi']\n",
      "\n",
      "One-Hot Encoded:\n",
      "   City_Dhaka  City_Chattogram  City_Rajshahi\n",
      "0           1                0              0\n",
      "1           0                1              0\n",
      "2           1                0              0\n",
      "3           0                0              1\n",
      "4           0                0              1\n"
     ]
    }
   ],
   "source": [
    "print(\"B2. One-Hot Encoding by Hand\\n\")\n",
    "Cities = ['Dhaka', 'Chattogram', 'Dhaka', 'Rajshahi', 'Rajshahi']\n",
    "print(f\"Original Cities: {Cities}\\n\")\n",
    "\n",
    "# Create one-hot encoding\n",
    "df_onehot = pd.DataFrame({\n",
    "    'City_Dhaka': [1 if c == 'Dhaka' else 0 for c in Cities],\n",
    "    'City_Chattogram': [1 if c == 'Chattogram' else 0 for c in Cities],\n",
    "    'City_Rajshahi': [1 if c == 'Rajshahi' else 0 for c in Cities]\n",
    "})\n",
    "\n",
    "print(\"One-Hot Encoded:\")\n",
    "print(df_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Ordinal mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B3. Ordinal Mapping\n",
      "\n",
      "Original Education: ['High School', 'Bachelor', 'Master', 'Bachelor', 'Master']\n",
      "\n",
      "Mapping 1 (High School=0, Bachelor=1, Master=2): [0, 1, 2, 1, 2]\n",
      "Distance Bachelor to Master: -1 = 1\n",
      "\n",
      "Mapping 2 (High School=1, Bachelor=2, Master=3): [1, 2, 3, 2, 3]\n",
      "Distance Bachelor to Master: -1 = 1\n",
      "\n",
      "Explanation: The distances between categories remain the same (still 1 unit apart),\n",
      "but the absolute values shift, which can affect intercepts in models but not slopes.\n"
     ]
    }
   ],
   "source": [
    "print(\"B3. Ordinal Mapping\\n\")\n",
    "Education = ['High School', 'Bachelor', 'Master', 'Bachelor', 'Master']\n",
    "print(f\"Original Education: {Education}\\n\")\n",
    "\n",
    "# First mapping: 0, 1, 2\n",
    "mapping1 = {'High School': 0, 'Bachelor': 1, 'Master': 2}\n",
    "encoded1 = [mapping1[e] for e in Education]\n",
    "print(f\"Mapping 1 (High School=0, Bachelor=1, Master=2): {encoded1}\")\n",
    "print(f\"Distance Bachelor to Master: {1 - 2} = {abs(1 - 2)}\\n\")\n",
    "\n",
    "# Second mapping: 1, 2, 3\n",
    "mapping2 = {'High School': 1, 'Bachelor': 2, 'Master': 3}\n",
    "encoded2 = [mapping2[e] for e in Education]\n",
    "print(f\"Mapping 2 (High School=1, Bachelor=2, Master=3): {encoded2}\")\n",
    "print(f\"Distance Bachelor to Master: {2 - 3} = {abs(2 - 3)}\\n\")\n",
    "\n",
    "print(\"Explanation: The distances between categories remain the same (still 1 unit apart),\")\n",
    "print(\"but the absolute values shift, which can affect intercepts in models but not slopes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4. Encoding mixup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B4. Encoding Mixup\n",
      "\n",
      "Risk of applying ordinal encoding to City and one-hot to Education:\n",
      "\n",
      "Ordinal encoding on City imposes a false ordering (e.g., Dhaka < Chattogram),\n",
      "making the linear model incorrectly assume cities have a numerical relationship,\n",
      "while one-hot on Education loses the natural ordering, treating Master and High School as equally different.\n"
     ]
    }
   ],
   "source": [
    "print(\"B4. Encoding Mixup\\n\")\n",
    "print(\"Risk of applying ordinal encoding to City and one-hot to Education:\")\n",
    "print(\"\\nOrdinal encoding on City imposes a false ordering (e.g., Dhaka < Chattogram),\")\n",
    "print(\"making the linear model incorrectly assume cities have a numerical relationship,\")\n",
    "print(\"while one-hot on Education loses the natural ordering, treating Master and High School as equally different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B5. Vectors and alignment (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B5. Vectors and Alignment\n",
      "\n",
      "a = [ 3 -1  2]\n",
      "b = [ 4  0 -2]\n",
      "c = [-6  2 -4]\n",
      "\n",
      "a) Dot products:\n",
      "   a · b = (3)(4) + (-1)(0) + (2)(-2) = 12 + 0 - 4 = 8\n",
      "   a · c = (3)(-6) + (-1)(2) + (2)(-4) = -18 - 2 - 8 = -28\n",
      "\n",
      "b) Alignment analysis:\n",
      "   a · b = 8 (positive but small) - vectors are slightly aligned\n",
      "   a · c = -28 (negative and large) - vectors point in opposite directions\n",
      "   Note: c = -2 * a, so they are perfectly anti-aligned\n",
      "\n",
      "c) L2 normalization:\n",
      "   ||a|| = sqrt(3^2 + (-1)^2 + 2^2) = sqrt(9 + 1 + 4) = sqrt(14) = 3.741657\n",
      "   a_normalized = a / ||a|| = [ 0.80178373 -0.26726124  0.53452248]\n",
      "   a_normalized (3 decimals) = [0.802, -0.267, 0.535]\n"
     ]
    }
   ],
   "source": [
    "print(\"B5. Vectors and Alignment\\n\")\n",
    "a = np.array([3, -1, 2])\n",
    "b = np.array([4, 0, -2])\n",
    "c = np.array([-6, 2, -4])\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"c = {c}\\n\")\n",
    "\n",
    "# a) Compute dot products\n",
    "dot_ab = np.dot(a, b)\n",
    "dot_ac = np.dot(a, c)\n",
    "print(f\"a) Dot products:\")\n",
    "print(f\"   a · b = (3)(4) + (-1)(0) + (2)(-2) = 12 + 0 - 4 = {dot_ab}\")\n",
    "print(f\"   a · c = (3)(-6) + (-1)(2) + (2)(-4) = -18 - 2 - 8 = {dot_ac}\\n\")\n",
    "\n",
    "# b) Compare signs and magnitudes\n",
    "print(f\"b) Alignment analysis:\")\n",
    "print(f\"   a · b = {dot_ab} (positive but small) - vectors are slightly aligned\")\n",
    "print(f\"   a · c = {dot_ac} (negative and large) - vectors point in opposite directions\")\n",
    "print(f\"   Note: c = -2 * a, so they are perfectly anti-aligned\\n\")\n",
    "\n",
    "# c) L2 normalize a\n",
    "norm_a = np.linalg.norm(a)\n",
    "a_normalized = a / norm_a\n",
    "print(f\"c) L2 normalization:\")\n",
    "print(f\"   ||a|| = sqrt(3^2 + (-1)^2 + 2^2) = sqrt(9 + 1 + 4) = sqrt(14) = {norm_a:.6f}\")\n",
    "print(f\"   a_normalized = a / ||a|| = {a_normalized}\")\n",
    "print(f\"   a_normalized (3 decimals) = [{a_normalized[0]:.3f}, {a_normalized[1]:.3f}, {a_normalized[2]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B6. Two distances, different vibes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B6. Two Distances, Different Vibes\n",
      "\n",
      "P1 = [2 3]\n",
      "P2 = [5 7]\n",
      "P3 = [ 2 10]\n",
      "\n",
      "a) Distances for all pairs:\n",
      "\n",
      "P1 to P2:\n",
      "  Euclidean = sqrt((5-2)^2 + (7-3)^2) = sqrt(9 + 16) = sqrt(25) = 5.0000\n",
      "  Manhattan = |5-2| + |7-3| = 3 + 4 = 7.0000\n",
      "\n",
      "P1 to P3:\n",
      "  Euclidean = sqrt((2-2)^2 + (10-3)^2) = sqrt(0 + 49) = 7.0000\n",
      "  Manhattan = |2-2| + |10-3| = 0 + 7 = 7.0000\n",
      "\n",
      "P2 to P3:\n",
      "  Euclidean = sqrt((2-5)^2 + (10-7)^2) = sqrt(9 + 9) = 4.2426\n",
      "  Manhattan = |2-5| + |10-7| = 3 + 3 = 6.0000\n",
      "\n",
      "b) Sensitivity to large jumps:\n",
      "   Manhattan distance is MORE sensitive to a single large jump in one coordinate\n",
      "   because it sums absolute differences, while Euclidean squares them (dampening effect).\n",
      "\n",
      "c) Effect of scaling y-coordinate by 10:\n",
      "\n",
      "Original P1 to P2: Euclidean = 5.0000, Manhattan = 7.0000\n",
      "Scaled P1 to P2:   Euclidean = 40.1123, Manhattan = 43.0000\n",
      "\n",
      "Explanation: Scaling y by 10 amplifies distances in the y-direction, making the\n",
      "y-component dominate both distance metrics, but Euclidean grows more dramatically due to squaring.\n"
     ]
    }
   ],
   "source": [
    "print(\"B6. Two Distances, Different Vibes\\n\")\n",
    "P1 = np.array([2, 3])\n",
    "P2 = np.array([5, 7])\n",
    "P3 = np.array([2, 10])\n",
    "\n",
    "print(f\"P1 = {P1}\")\n",
    "print(f\"P2 = {P2}\")\n",
    "print(f\"P3 = {P3}\\n\")\n",
    "\n",
    "# a) Compute both distances for all pairs\n",
    "print(\"a) Distances for all pairs:\\n\")\n",
    "\n",
    "euclidean_p1_p2 = distance.euclidean(P1, P2)\n",
    "manhattan_p1_p2 = distance.cityblock(P1, P2)\n",
    "print(f\"P1 to P2:\")\n",
    "print(f\"  Euclidean = sqrt((5-2)^2 + (7-3)^2) = sqrt(9 + 16) = sqrt(25) = {euclidean_p1_p2:.4f}\")\n",
    "print(f\"  Manhattan = |5-2| + |7-3| = 3 + 4 = {manhattan_p1_p2:.4f}\\n\")\n",
    "\n",
    "euclidean_p1_p3 = distance.euclidean(P1, P3)\n",
    "manhattan_p1_p3 = distance.cityblock(P1, P3)\n",
    "print(f\"P1 to P3:\")\n",
    "print(f\"  Euclidean = sqrt((2-2)^2 + (10-3)^2) = sqrt(0 + 49) = {euclidean_p1_p3:.4f}\")\n",
    "print(f\"  Manhattan = |2-2| + |10-3| = 0 + 7 = {manhattan_p1_p3:.4f}\\n\")\n",
    "\n",
    "euclidean_p2_p3 = distance.euclidean(P2, P3)\n",
    "manhattan_p2_p3 = distance.cityblock(P2, P3)\n",
    "print(f\"P2 to P3:\")\n",
    "print(f\"  Euclidean = sqrt((2-5)^2 + (10-7)^2) = sqrt(9 + 9) = {euclidean_p2_p3:.4f}\")\n",
    "print(f\"  Manhattan = |2-5| + |10-7| = 3 + 3 = {manhattan_p2_p3:.4f}\\n\")\n",
    "\n",
    "# b) Which distance is more sensitive\n",
    "print(\"b) Sensitivity to large jumps:\")\n",
    "print(\"   Manhattan distance is MORE sensitive to a single large jump in one coordinate\")\n",
    "print(\"   because it sums absolute differences, while Euclidean squares them (dampening effect).\\n\")\n",
    "\n",
    "# c) Scale y by 10 and recompute\n",
    "print(\"c) Effect of scaling y-coordinate by 10:\\n\")\n",
    "P1_scaled = np.array([2, 3*10])\n",
    "P2_scaled = np.array([5, 7*10])\n",
    "\n",
    "euclidean_scaled = distance.euclidean(P1_scaled, P2_scaled)\n",
    "manhattan_scaled = distance.cityblock(P1_scaled, P2_scaled)\n",
    "\n",
    "print(f\"Original P1 to P2: Euclidean = {euclidean_p1_p2:.4f}, Manhattan = {manhattan_p1_p2:.4f}\")\n",
    "print(f\"Scaled P1 to P2:   Euclidean = {euclidean_scaled:.4f}, Manhattan = {manhattan_scaled:.4f}\\n\")\n",
    "\n",
    "print(\"Explanation: Scaling y by 10 amplifies distances in the y-direction, making the\")\n",
    "print(\"y-component dominate both distance metrics, but Euclidean grows more dramatically due to squaring.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Mini Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-Data-1:\n",
      "   ID  Age  Hours_Study  GPA Internet        City\n",
      "0   1   20          1.0  3.1      Yes       Dhaka\n",
      "1   2   21          0.5  2.6       No  Chattogram\n",
      "2   3   22          2.2  3.4      Yes    Rajshahi\n",
      "3   4   20          5.0  3.9      Yes       Dhaka\n",
      "4   5   23          0.2  2.3       No    Rajshahi\n",
      "\n",
      "C-Data-2:\n",
      "   ID  Income_BDT  Transactions  Temp_C    Education Satisfaction\n",
      "0   1       30000             0    25.0  High School          Low\n",
      "1   2       45000             1    26.0     Bachelor       Medium\n",
      "2   3       52000             2    24.5       Master         High\n",
      "3   4      300000            12    28.0     Bachelor       Medium\n",
      "4   5       38000             0    25.5       Master       Medium\n"
     ]
    }
   ],
   "source": [
    "# Create C-Data-1\n",
    "c_data_1 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Age': [20, 21, 22, 20, 23],\n",
    "    'Hours_Study': [1.0, 0.5, 2.2, 5.0, 0.2],\n",
    "    'GPA': [3.10, 2.60, 3.40, 3.90, 2.30],\n",
    "    'Internet': ['Yes', 'No', 'Yes', 'Yes', 'No'],\n",
    "    'City': ['Dhaka', 'Chattogram', 'Rajshahi', 'Dhaka', 'Rajshahi']\n",
    "})\n",
    "\n",
    "# Create C-Data-2\n",
    "c_data_2 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Income_BDT': [30000, 45000, 52000, 300000, 38000],\n",
    "    'Transactions': [0, 1, 2, 12, 0],\n",
    "    'Temp_C': [25.0, 26.0, 24.5, 28.0, 25.5],\n",
    "    'Education': ['High School', 'Bachelor', 'Master', 'Bachelor', 'Master'],\n",
    "    'Satisfaction': ['Low', 'Medium', 'High', 'Medium', 'Medium']\n",
    "})\n",
    "\n",
    "print(\"C-Data-1:\")\n",
    "print(c_data_1)\n",
    "print(\"\\nC-Data-2:\")\n",
    "print(c_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1. Scaler choices with evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1. Scaler Choices with Evidence\n",
      "\n",
      "1. Income_BDT:\n",
      "   Scaler: ROBUST SCALER\n",
      "   Justification: Contains outlier (300000 vs 30000-52000 range)\n",
      "\n",
      "   Numeric illustration:\n",
      "   Original: [ 30000  45000  52000 300000  38000]\n",
      "   Robust scaled: [-1.0714  0.      0.5    18.2143 -0.5   ]\n",
      "   Notice outlier 300000 becomes 18.2143, not extremely far from others\n",
      "\n",
      "2. Transactions:\n",
      "   Scaler: ROBUST SCALER\n",
      "   Justification: Has outlier (12 vs mostly 0-2) and many zeros\n",
      "\n",
      "   Numeric illustration:\n",
      "   Original: [ 0  1  2 12  0]\n",
      "   Robust scaled: [-0.5  0.   0.5  5.5 -0.5]\n",
      "   Power user (12) is handled without dominating the scale\n",
      "\n",
      "3. Temp_C:\n",
      "   Scaler: MIN-MAX SCALER\n",
      "   Justification: Bounded range (24.5-28.0), no outliers, natural physical bounds\n",
      "\n",
      "   Numeric illustration:\n",
      "   Original: [25.  26.  24.5 28.  25.5]\n",
      "   Min-Max scaled: [0.1429 0.4286 0.     1.     0.2857]\n",
      "   All values fit nicely in [0, 1] with no distortion from outliers\n"
     ]
    }
   ],
   "source": [
    "print(\"C1. Scaler Choices with Evidence\\n\")\n",
    "\n",
    "# Income_BDT\n",
    "print(\"1. Income_BDT:\")\n",
    "print(\"   Scaler: ROBUST SCALER\")\n",
    "print(\"   Justification: Contains outlier (300000 vs 30000-52000 range)\\n\")\n",
    "income = c_data_2['Income_BDT'].values\n",
    "median_inc = np.median(income)\n",
    "q1_inc = np.percentile(income, 25)\n",
    "q3_inc = np.percentile(income, 75)\n",
    "iqr_inc = q3_inc - q1_inc\n",
    "income_robust = (income - median_inc) / iqr_inc\n",
    "print(f\"   Numeric illustration:\")\n",
    "print(f\"   Original: {income}\")\n",
    "print(f\"   Robust scaled: {income_robust.round(4)}\")\n",
    "print(f\"   Notice outlier 300000 becomes {income_robust[3]:.4f}, not extremely far from others\\n\")\n",
    "\n",
    "# Transactions\n",
    "print(\"2. Transactions:\")\n",
    "print(\"   Scaler: ROBUST SCALER\")\n",
    "print(\"   Justification: Has outlier (12 vs mostly 0-2) and many zeros\\n\")\n",
    "trans = c_data_2['Transactions'].values\n",
    "median_trans = np.median(trans)\n",
    "q1_trans = np.percentile(trans, 25)\n",
    "q3_trans = np.percentile(trans, 75)\n",
    "iqr_trans = q3_trans - q1_trans\n",
    "trans_robust = (trans - median_trans) / iqr_trans if iqr_trans != 0 else trans - median_trans\n",
    "print(f\"   Numeric illustration:\")\n",
    "print(f\"   Original: {trans}\")\n",
    "print(f\"   Robust scaled: {trans_robust.round(4)}\")\n",
    "print(f\"   Power user (12) is handled without dominating the scale\\n\")\n",
    "\n",
    "# Temp_C\n",
    "print(\"3. Temp_C:\")\n",
    "print(\"   Scaler: MIN-MAX SCALER\")\n",
    "print(\"   Justification: Bounded range (24.5-28.0), no outliers, natural physical bounds\\n\")\n",
    "temp = c_data_2['Temp_C'].values\n",
    "temp_min = temp.min()\n",
    "temp_max = temp.max()\n",
    "temp_minmax = (temp - temp_min) / (temp_max - temp_min)\n",
    "print(f\"   Numeric illustration:\")\n",
    "print(f\"   Original: {temp}\")\n",
    "print(f\"   Min-Max scaled: {temp_minmax.round(4)}\")\n",
    "print(f\"   All values fit nicely in [0, 1] with no distortion from outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2. Mixed preprocessing plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2. Mixed Preprocessing Plan\n",
      "\n",
      "a) Categorization:\n",
      "\n",
      "NOMINAL columns:\n",
      "  - Internet (Yes/No - no ordering)\n",
      "  - City (Dhaka, Chattogram, Rajshahi - no ordering)\n",
      "\n",
      "ORDINAL columns:\n",
      "  - Education (High School < Bachelor < Master)\n",
      "  - Satisfaction (Low < Medium < High)\n",
      "\n",
      "b) Encoding Plan:\n",
      "\n",
      "ONE-HOT ENCODING:\n",
      "  - Internet\n",
      "  - City\n",
      "\n",
      "ORDINAL ENCODING:\n",
      "  - Education: High School=0, Bachelor=1, Master=2\n",
      "  - Satisfaction: Low=0, Medium=1, High=2\n",
      "\n",
      "c) Scaling Plan:\n",
      "\n",
      "MIN-MAX SCALING:\n",
      "  - Age (small range, no outliers: 20-23)\n",
      "  - GPA (bounded: 2.30-3.90, no outliers)\n",
      "  - Temp_C (physical bounds: 24.5-28.0)\n",
      "\n",
      "ROBUST SCALING:\n",
      "  - Income_BDT (has extreme outlier: 300000)\n",
      "  - Transactions (has outlier: 12 vs mostly 0-2)\n",
      "  - Hours_Study (has extreme values: 5.0 vs mostly < 2.5)\n",
      "\n",
      "STANDARDIZATION:\n",
      "  - (None in this case - use Min-Max for bounded or Robust for outliers)\n"
     ]
    }
   ],
   "source": [
    "print(\"C2. Mixed Preprocessing Plan\\n\")\n",
    "\n",
    "# a) Identify nominal and ordinal\n",
    "print(\"a) Categorization:\\n\")\n",
    "print(\"NOMINAL columns:\")\n",
    "print(\"  - Internet (Yes/No - no ordering)\")\n",
    "print(\"  - City (Dhaka, Chattogram, Rajshahi - no ordering)\\n\")\n",
    "\n",
    "print(\"ORDINAL columns:\")\n",
    "print(\"  - Education (High School < Bachelor < Master)\")\n",
    "print(\"  - Satisfaction (Low < Medium < High)\\n\")\n",
    "\n",
    "# b) Encoding plan\n",
    "print(\"b) Encoding Plan:\\n\")\n",
    "print(\"ONE-HOT ENCODING:\")\n",
    "print(\"  - Internet\")\n",
    "print(\"  - City\\n\")\n",
    "\n",
    "print(\"ORDINAL ENCODING:\")\n",
    "print(\"  - Education: High School=0, Bachelor=1, Master=2\")\n",
    "print(\"  - Satisfaction: Low=0, Medium=1, High=2\\n\")\n",
    "\n",
    "# c) Scaling plan\n",
    "print(\"c) Scaling Plan:\\n\")\n",
    "print(\"MIN-MAX SCALING:\")\n",
    "print(\"  - Age (small range, no outliers: 20-23)\")\n",
    "print(\"  - GPA (bounded: 2.30-3.90, no outliers)\")\n",
    "print(\"  - Temp_C (physical bounds: 24.5-28.0)\\n\")\n",
    "\n",
    "print(\"ROBUST SCALING:\")\n",
    "print(\"  - Income_BDT (has extreme outlier: 300000)\")\n",
    "print(\"  - Transactions (has outlier: 12 vs mostly 0-2)\")\n",
    "print(\"  - Hours_Study (has extreme values: 5.0 vs mostly < 2.5)\\n\")\n",
    "\n",
    "print(\"STANDARDIZATION:\")\n",
    "print(\"  - (None in this case - use Min-Max for bounded or Robust for outliers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3. Outlier stress test (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3. Outlier Stress Test\n",
      "\n",
      "Income_BDT: [ 30000  45000  52000 300000  38000]\n",
      "\n",
      "Min-Max Scaled:\n",
      "[0.     0.0556 0.0815 1.     0.0296]\n",
      "\n",
      "Robust Scaled:\n",
      "[-1.0714  0.      0.5    18.2143 -0.5   ]\n",
      "\n",
      "Comparison:\n",
      "Min-Max compresses normal values to [0, 0.08] while outlier becomes 1.0 (dominates scale);\n",
      "Robust keeps outlier at 18.21, more proportional to actual deviation from median.\n"
     ]
    }
   ],
   "source": [
    "print(\"C3. Outlier Stress Test\\n\")\n",
    "income = c_data_2['Income_BDT'].values\n",
    "print(f\"Income_BDT: {income}\\n\")\n",
    "\n",
    "# Min-Max scaling\n",
    "income_min = income.min()\n",
    "income_max = income.max()\n",
    "income_minmax = (income - income_min) / (income_max - income_min)\n",
    "print(\"Min-Max Scaled:\")\n",
    "print(income_minmax.round(4))\n",
    "\n",
    "# Robust scaling\n",
    "income_median = np.median(income)\n",
    "income_q1 = np.percentile(income, 25)\n",
    "income_q3 = np.percentile(income, 75)\n",
    "income_iqr = income_q3 - income_q1\n",
    "income_robust = (income - income_median) / income_iqr\n",
    "print(\"\\nRobust Scaled:\")\n",
    "print(income_robust.round(4))\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Min-Max compresses normal values to [0, 0.08] while outlier becomes 1.0 (dominates scale);\")\n",
    "print(f\"Robust keeps outlier at {income_robust[3]:.2f}, more proportional to actual deviation from median.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4. Distance on feature space (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4. Distance on Feature Space\n",
      "\n",
      "ID 1: Hours_Study=1.0, GPA=3.1\n",
      "ID 4: Hours_Study=5.0, GPA=3.9\n",
      "\n",
      "a) Euclidean distance:\n",
      "   sqrt((5.0-1.0)^2 + (3.90-3.10)^2) = sqrt(16 + 0.64) = sqrt(16.64) = 4.0792\n",
      "\n",
      "b) Manhattan distance:\n",
      "   |5.0-1.0| + |3.90-3.10| = 4.0 + 0.8 = 4.8000\n",
      "\n",
      "c) After Min-Max normalization:\n",
      "\n",
      "ID 1 normalized: [0.16666667 0.5       ]\n",
      "ID 4 normalized: [1. 1.]\n",
      "\n",
      "Euclidean distance (normalized): 0.9718\n",
      "Manhattan distance (normalized): 1.3333\n",
      "\n",
      "Comment: Normalization prevents Hours_Study (range ~5) from dominating GPA (range ~1.6),\n",
      "making both features contribute equally to distance calculations.\n"
     ]
    }
   ],
   "source": [
    "print(\"C4. Distance on Feature Space\\n\")\n",
    "\n",
    "# Extract features\n",
    "hours_study = c_data_1['Hours_Study'].values\n",
    "gpa = c_data_1['GPA'].values\n",
    "\n",
    "# ID 1 and ID 4\n",
    "id1_features = np.array([hours_study[0], gpa[0]])  # [1.0, 3.10]\n",
    "id4_features = np.array([hours_study[3], gpa[3]])  # [5.0, 3.90]\n",
    "\n",
    "print(f\"ID 1: Hours_Study={id1_features[0]}, GPA={id1_features[1]}\")\n",
    "print(f\"ID 4: Hours_Study={id4_features[0]}, GPA={id4_features[1]}\\n\")\n",
    "\n",
    "# a) Euclidean distance\n",
    "euclidean_orig = distance.euclidean(id1_features, id4_features)\n",
    "print(f\"a) Euclidean distance:\")\n",
    "print(f\"   sqrt((5.0-1.0)^2 + (3.90-3.10)^2) = sqrt(16 + 0.64) = sqrt(16.64) = {euclidean_orig:.4f}\\n\")\n",
    "\n",
    "# b) Manhattan distance\n",
    "manhattan_orig = distance.cityblock(id1_features, id4_features)\n",
    "print(f\"b) Manhattan distance:\")\n",
    "print(f\"   |5.0-1.0| + |3.90-3.10| = 4.0 + 0.8 = {manhattan_orig:.4f}\\n\")\n",
    "\n",
    "# c) Normalize and recompute\n",
    "print(\"c) After Min-Max normalization:\\n\")\n",
    "\n",
    "# Min-Max normalize\n",
    "hours_min, hours_max = hours_study.min(), hours_study.max()\n",
    "gpa_min, gpa_max = gpa.min(), gpa.max()\n",
    "\n",
    "hours_normalized = (hours_study - hours_min) / (hours_max - hours_min)\n",
    "gpa_normalized = (gpa - gpa_min) / (gpa_max - gpa_min)\n",
    "\n",
    "id1_normalized = np.array([hours_normalized[0], gpa_normalized[0]])\n",
    "id4_normalized = np.array([hours_normalized[3], gpa_normalized[3]])\n",
    "\n",
    "print(f\"ID 1 normalized: {id1_normalized}\")\n",
    "print(f\"ID 4 normalized: {id4_normalized}\\n\")\n",
    "\n",
    "euclidean_norm = distance.euclidean(id1_normalized, id4_normalized)\n",
    "manhattan_norm = distance.cityblock(id1_normalized, id4_normalized)\n",
    "\n",
    "print(f\"Euclidean distance (normalized): {euclidean_norm:.4f}\")\n",
    "print(f\"Manhattan distance (normalized): {manhattan_norm:.4f}\\n\")\n",
    "\n",
    "print(\"Comment: Normalization prevents Hours_Study (range ~5) from dominating GPA (range ~1.6),\")\n",
    "print(\"making both features contribute equally to distance calculations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: Mini Project (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a small DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Project DataFrame:\n",
      "   Income  Hours_Study  GPA        City Internet Education_Level Satisfaction\n",
      "0   35000          2.5  3.2       Dhaka      Yes        Bachelor       Medium\n",
      "1   48000          3.0  3.5  Chattogram      Yes          Master         High\n",
      "2  250000          1.5  2.8       Dhaka       No     High School          Low\n",
      "3   42000          4.5  3.8    Rajshahi      Yes          Master         High\n",
      "4   38000          2.0  3.1       Dhaka      Yes        Bachelor       Medium\n",
      "5   55000          3.5  3.6  Chattogram       No          Master         High\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic dataset\n",
    "df_project = pd.DataFrame({\n",
    "    'Income': [35000, 48000, 250000, 42000, 38000, 55000],\n",
    "    'Hours_Study': [2.5, 3.0, 1.5, 4.5, 2.0, 3.5],\n",
    "    'GPA': [3.2, 3.5, 2.8, 3.8, 3.1, 3.6],\n",
    "    'City': ['Dhaka', 'Chattogram', 'Dhaka', 'Rajshahi', 'Dhaka', 'Chattogram'],\n",
    "    'Internet': ['Yes', 'Yes', 'No', 'Yes', 'Yes', 'No'],\n",
    "    'Education_Level': ['Bachelor', 'Master', 'High School', 'Master', 'Bachelor', 'Master'],\n",
    "    'Satisfaction': ['Medium', 'High', 'Low', 'High', 'Medium', 'High']\n",
    "})\n",
    "\n",
    "print(\"Mini Project DataFrame:\")\n",
    "print(df_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing Plan\n",
    "\n",
    "**One-Hot Encoding:**\n",
    "- City (nominal: Dhaka, Chattogram, Rajshahi)\n",
    "- Internet (nominal: Yes, No)\n",
    "\n",
    "**Ordinal Encoding:**\n",
    "- Education_Level: High School=0, Bachelor=1, Master=2\n",
    "- Satisfaction: Low=0, Medium=1, High=2\n",
    "\n",
    "**Scaling:**\n",
    "- Income: Robust Scaler (has outlier: 250000)\n",
    "- Hours_Study: Standardization (normal distribution)\n",
    "- GPA: Min-Max Scaler (bounded range, no outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed array shape: (6, 8)\n",
      "\n",
      "First few rows of transformed data:\n",
      "[[ 1.          0.          1.          1.          1.         -0.70175439\n",
      "  -0.3380617   0.4       ]\n",
      " [ 0.          0.          1.          2.          2.          0.21052632\n",
      "   0.16903085  0.7       ]\n",
      " [ 1.          0.          0.          0.          0.         14.38596491\n",
      "  -1.35224681  0.        ]]\n",
      "\n",
      "Feature names: ['City_Dhaka', 'City_Rajshahi', 'Internet_Yes', 'Education_Level', 'Satisfaction', 'Income', 'Hours_Study', 'GPA']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define column groups\n",
    "onehot_cols = ['City', 'Internet']\n",
    "ordinal_cols = ['Education_Level', 'Satisfaction']\n",
    "robust_cols = ['Income']\n",
    "standard_cols = ['Hours_Study']\n",
    "minmax_cols = ['GPA']\n",
    "\n",
    "# Create transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False), onehot_cols),\n",
    "        ('ordinal_education', OrdinalEncoder(categories=[['High School', 'Bachelor', 'Master']]), ['Education_Level']),\n",
    "        ('ordinal_satisfaction', OrdinalEncoder(categories=[['Low', 'Medium', 'High']]), ['Satisfaction']),\n",
    "        ('robust', RobustScaler(), robust_cols),\n",
    "        ('standard', StandardScaler(), standard_cols),\n",
    "        ('minmax', MinMaxScaler(), minmax_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply transformation\n",
    "transformed = preprocessor.fit_transform(df_project)\n",
    "\n",
    "print(f\"Transformed array shape: {transformed.shape}\\n\")\n",
    "print(\"First few rows of transformed data:\")\n",
    "print(transformed[:3])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = (preprocessor.named_transformers_['onehot'].get_feature_names_out(onehot_cols).tolist() +\n",
    "                ['Education_Level', 'Satisfaction', 'Income', 'Hours_Study', 'GPA'])\n",
    "print(f\"\\nFeature names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Distance before vs after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Points:\n",
      "P1: [3.5e+04 2.5e+00]\n",
      "P2: [4.8e+04 3.0e+00]\n",
      "P3: [2.5e+05 1.5e+00]\n",
      "\n",
      "Distances BEFORE scaling:\n",
      "Euclidean: P1-P2=13000.00, P1-P3=215000.00, P2-P3=202000.00\n",
      "Manhattan: P1-P2=13000.50, P1-P3=215001.00, P2-P3=202001.50\n",
      "\n",
      "Distances AFTER Standard Scaling:\n",
      "Euclidean: P1-P2=0.81, P1-P3=2.71, P2-P3=3.16\n",
      "Manhattan: P1-P2=0.93, P1-P3=3.79, P2-P3=4.46\n",
      "\n",
      "Distances AFTER Robust Scaling:\n",
      "Euclidean: P1-P2=0.68, P1-P3=2.40, P2-P3=2.74\n",
      "Manhattan: P1-P2=0.79, P1-P3=3.33, P2-P3=3.88\n"
     ]
    }
   ],
   "source": [
    "# Select Income and Hours_Study for distance analysis\n",
    "# Pick rows 0, 1, 2 as P1, P2, P3\n",
    "P1 = df_project[['Income', 'Hours_Study']].iloc[0].values\n",
    "P2 = df_project[['Income', 'Hours_Study']].iloc[1].values\n",
    "P3 = df_project[['Income', 'Hours_Study']].iloc[2].values\n",
    "\n",
    "print(\"Original Points:\")\n",
    "print(f\"P1: {P1}\")\n",
    "print(f\"P2: {P2}\")\n",
    "print(f\"P3: {P3}\\n\")\n",
    "\n",
    "# Distances before scaling\n",
    "print(\"Distances BEFORE scaling:\")\n",
    "eucl_p1_p2_orig = distance.euclidean(P1, P2)\n",
    "eucl_p1_p3_orig = distance.euclidean(P1, P3)\n",
    "eucl_p2_p3_orig = distance.euclidean(P2, P3)\n",
    "manh_p1_p2_orig = distance.cityblock(P1, P2)\n",
    "manh_p1_p3_orig = distance.cityblock(P1, P3)\n",
    "manh_p2_p3_orig = distance.cityblock(P2, P3)\n",
    "\n",
    "print(f\"Euclidean: P1-P2={eucl_p1_p2_orig:.2f}, P1-P3={eucl_p1_p3_orig:.2f}, P2-P3={eucl_p2_p3_orig:.2f}\")\n",
    "print(f\"Manhattan: P1-P2={manh_p1_p2_orig:.2f}, P1-P3={manh_p1_p3_orig:.2f}, P2-P3={manh_p2_p3_orig:.2f}\\n\")\n",
    "\n",
    "# Apply Standard Scaler\n",
    "scaler_std = StandardScaler()\n",
    "data_std = scaler_std.fit_transform(df_project[['Income', 'Hours_Study']].iloc[:3])\n",
    "P1_std, P2_std, P3_std = data_std[0], data_std[1], data_std[2]\n",
    "\n",
    "print(\"Distances AFTER Standard Scaling:\")\n",
    "eucl_p1_p2_std = distance.euclidean(P1_std, P2_std)\n",
    "eucl_p1_p3_std = distance.euclidean(P1_std, P3_std)\n",
    "eucl_p2_p3_std = distance.euclidean(P2_std, P3_std)\n",
    "manh_p1_p2_std = distance.cityblock(P1_std, P2_std)\n",
    "manh_p1_p3_std = distance.cityblock(P1_std, P3_std)\n",
    "manh_p2_p3_std = distance.cityblock(P2_std, P3_std)\n",
    "\n",
    "print(f\"Euclidean: P1-P2={eucl_p1_p2_std:.2f}, P1-P3={eucl_p1_p3_std:.2f}, P2-P3={eucl_p2_p3_std:.2f}\")\n",
    "print(f\"Manhattan: P1-P2={manh_p1_p2_std:.2f}, P1-P3={manh_p1_p3_std:.2f}, P2-P3={manh_p2_p3_std:.2f}\\n\")\n",
    "\n",
    "# Apply Robust Scaler\n",
    "scaler_robust = RobustScaler()\n",
    "data_robust = scaler_robust.fit_transform(df_project[['Income', 'Hours_Study']].iloc[:3])\n",
    "P1_robust, P2_robust, P3_robust = data_robust[0], data_robust[1], data_robust[2]\n",
    "\n",
    "print(\"Distances AFTER Robust Scaling:\")\n",
    "eucl_p1_p2_robust = distance.euclidean(P1_robust, P2_robust)\n",
    "eucl_p1_p3_robust = distance.euclidean(P1_robust, P3_robust)\n",
    "eucl_p2_p3_robust = distance.euclidean(P2_robust, P3_robust)\n",
    "manh_p1_p2_robust = distance.cityblock(P1_robust, P2_robust)\n",
    "manh_p1_p3_robust = distance.cityblock(P1_robust, P3_robust)\n",
    "manh_p2_p3_robust = distance.cityblock(P2_robust, P3_robust)\n",
    "\n",
    "print(f\"Euclidean: P1-P2={eucl_p1_p2_robust:.2f}, P1-P3={eucl_p1_p3_robust:.2f}, P2-P3={eucl_p2_p3_robust:.2f}\")\n",
    "print(f\"Manhattan: P1-P2={manh_p1_p2_robust:.2f}, P1-P3={manh_p1_p3_robust:.2f}, P2-P3={manh_p2_p3_robust:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Comparison Table\n",
    "\n",
    "| Pair | Original Eucl | Original Manh | Standard Eucl | Standard Manh | Robust Eucl | Robust Manh |\n",
    "|------|---------------|---------------|---------------|---------------|-------------|-------------|\n",
    "| P1-P2 | See above | See above | See above | See above | See above | See above |\n",
    "| P1-P3 | See above | See above | See above | See above | See above | See above |\n",
    "| P2-P3 | See above | See above | See above | See above | See above | See above |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Short Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REFLECTION:\n",
      "\n",
      "1. Which scaler handled outliers better?\n",
      "   Robust Scaler handled the Income outlier (250000) better than Standard Scaler.\n",
      "   It uses median and IQR, making it resistant to extreme values, while Standard\n",
      "   Scaler's mean and std are affected by outliers.\n",
      "\n",
      "2. Did scaling change which points are closer?\n",
      "   Yes, scaling changed relative distances. Before scaling, Income dominated due\n",
      "   to its large magnitude. After scaling, both features contribute equally,\n",
      "   revealing true similarity patterns based on both dimensions.\n",
      "\n",
      "3. Why does this matter for distance-based algorithms?\n",
      "   Distance-based algorithms (KNN, K-Means, SVM) are sensitive to feature scales.\n",
      "   Without proper scaling, features with larger ranges dominate distance calculations,\n",
      "   leading to biased results. Scaling ensures all features contribute fairly to\n",
      "   similarity measurements, improving model performance and interpretability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"REFLECTION:\n",
    "\n",
    "1. Which scaler handled outliers better?\n",
    "   Robust Scaler handled the Income outlier (250000) better than Standard Scaler.\n",
    "   It uses median and IQR, making it resistant to extreme values, while Standard\n",
    "   Scaler's mean and std are affected by outliers.\n",
    "\n",
    "2. Did scaling change which points are closer?\n",
    "   Yes, scaling changed relative distances. Before scaling, Income dominated due\n",
    "   to its large magnitude. After scaling, both features contribute equally,\n",
    "   revealing true similarity patterns based on both dimensions.\n",
    "\n",
    "3. Why does this matter for distance-based algorithms?\n",
    "   Distance-based algorithms (KNN, K-Means, SVM) are sensitive to feature scales.\n",
    "   Without proper scaling, features with larger ranges dominate distance calculations,\n",
    "   leading to biased results. Scaling ensures all features contribute fairly to\n",
    "   similarity measurements, improving model performance and interpretability.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook covered:\n",
    "- Different scaling techniques (Min-Max, Standardization, Robust)\n",
    "- Encoding strategies (One-Hot for nominal, Ordinal for ordered categories)\n",
    "- Vector operations (dot products, norms, normalization)\n",
    "- Distance metrics (Euclidean and Manhattan)\n",
    "- Practical preprocessing with ColumnTransformer\n",
    "- Impact of scaling on distance-based algorithms\n",
    "\n",
    "Key Takeaways:\n",
    "- Use Robust Scaler for data with outliers\n",
    "- Use Min-Max for bounded data without outliers\n",
    "- Use Standardization for normally distributed data\n",
    "- Always scale features before using distance-based algorithms\n",
    "- Choose encoding based on whether categories have natural ordering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
