{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116da01f",
   "metadata": {
    "id": "116da01f"
   },
   "source": [
    "# Module 09: Data Preprocessing and Feature Engineering\n",
    "\n",
    "This notebook supports **Module 09** and contains all code and explanatory text for:\n",
    "\n",
    "- **Part 1**: Missing values, encoding categorical variables, scaling/normalization\n",
    "- **Part 2**: Outlier detection, feature transformation, domain-driven features,\n",
    "  preprocessing pipelines, and a quick sanity-check model\n",
    "\n",
    "Datasets used:\n",
    "- **Titanic dataset** (from Kaggle) – for demonstrating missing values\n",
    "- **Heart Failure / Heart Disease dataset** – for the main preprocessing pipeline\n",
    "\n",
    "Please upload or mount the CSV files in your environment as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549dde5",
   "metadata": {
    "id": "c549dde5"
   },
   "source": [
    "---\n",
    "## Part 1: Core Preprocessing Concepts\n",
    "\n",
    "In Part 1 we cover:\n",
    "- Why preprocessing is needed\n",
    "- How to handle missing values\n",
    "- How to encode categorical variables\n",
    "- How to scale / normalize numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e69f56",
   "metadata": {
    "id": "49e69f56"
   },
   "source": [
    "###Handling Missing Values (Titanic Dataset)\n",
    "\n",
    "Real-life analogy: **attendance sheet with blank cells**.\n",
    "\n",
    "- Some students have `P` (present), some have `A` (absent), and some cells are blank.\n",
    "- If we ignore those blanks, the final attendance calculation will be wrong.\n",
    "- We must decide how to handle the blanks using logic.\n",
    "\n",
    "In the Titanic dataset:\n",
    "- `Age` has missing values (numeric)\n",
    "- `Embarked` has a few missing values (categorical)\n",
    "- `Cabin` has many missing values (often dropped in simple demos)\n",
    "\n",
    "We will:\n",
    "1. Inspect missing values\n",
    "2. Fill numeric column (`Age`) with the **median**\n",
    "3. Fill categorical column (`Embarked`) with the **mode**\n",
    "4. Drop `Cabin` because it is mostly missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcb9d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fbcb9d0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334589696,
     "user_tz": -360,
     "elapsed": 384,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "5d29063d-424c-4f56-a7b4-f46b0c8b2c37"
   },
   "outputs": [],
   "source": "import pandas as pd\n\n# Load Titanic dataset\n# NOTE: Make sure titanic.csv exists at this path or update the path accordingly.\ntitanic_path = \"/content/sample_data/Titanic-Dataset.csv\"  # change if needed\ndf_titanic = pd.read_csv(titanic_path)\n\n# Display the first 10 rows to understand the dataset structure\nprint(\"First 10 rows of Titanic dataset:\")\ndf_titanic.head(10)"
  },
  {
   "cell_type": "code",
   "source": "# Check the dimensions of our dataset (rows, columns)\ndf_titanic.shape",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34svfu8ZRbro",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334589701,
     "user_tz": -360,
     "elapsed": 3,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "3769373b-a949-45e4-9294-705bbf7f8c39"
   },
   "id": "34svfu8ZRbro",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Count unique values in each column to understand data types and potential categories\nprint(\"Unique values per column:\")\ndf_titanic.nunique()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIGbRKtGG70w",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334590034,
     "user_tz": -360,
     "elapsed": 332,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "6594863c-1b1b-4691-902a-573e36ffb008"
   },
   "id": "LIGbRKtGG70w",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check for missing values in each column (NaN/null values)\nprint(\"Missing values per column:\")\ndf_titanic.isnull().sum()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PwTMw-YhF9Uv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334590321,
     "user_tz": -360,
     "elapsed": 286,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "7e4710df-67bd-489d-a9d9-f5f17903a205"
   },
   "id": "PwTMw-YhF9Uv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Distribution of Age Column:\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram to visualize the Age distribution\n# This helps us understand if the data is skewed and which imputation method to use\nplt.figure(figsize=(8,6))\nsns.histplot(df_titanic[\"Age\"], kde = True, bins = 20)\nplt.title(\"Age Distribution of Titanic Passenger\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Frquencey\")\nplt.show()\n# We will use Median as the feature values are right skewed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9gg-D-iPg36",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334590719,
     "user_tz": -360,
     "elapsed": 396,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "81196d76-1cb4-460e-b2e1-47d0edd5ab37"
   },
   "id": "r9gg-D-iPg36",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Distribution of Embarked Column:\")\n# Create a count plot to see the distribution of embarkation ports\n# This helps us understand the most common port for imputation\nplt.figure(figsize=(8,6))\nsns.countplot(data=df_titanic, x=\"Embarked\")\nplt.title(\"Embarked Distribution of Titanic Passenger\")\nplt.xlabel(\"Embarked\")\nplt.ylabel(\"Frquencey\")\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVKydYexPqNv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334591507,
     "user_tz": -360,
     "elapsed": 787,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "5833de8c-887d-44cc-a9aa-461244675bf6"
   },
   "id": "WVKydYexPqNv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19822ec6",
   "metadata": {
    "id": "19822ec6"
   },
   "outputs": [],
   "source": "# WHY WE HANDLE MISSING VALUES:\n# 1. Missing values break most machine learning algorithms\n# 2. They reduce the statistical power of our dataset\n# 3. They can introduce bias if not handled properly\n# 4. Most algorithms cannot process NaN/null values\n\n# WHY DIFFERENT STRATEGIES FOR DIFFERENT DATA TYPES:\n\n# 1. Fill numeric missing values (Age) with MEDIAN\n# WHY MEDIAN? NOT MEAN?\n# - Age data is skewed (not normally distributed)\n# - Mean is sensitive to extreme values (outliers)\n# - Median is more robust to outliers\n# - Example: Ages [22, 25, 28, 30, 90] → Mean=39, Median=28\n#   The 90-year-old outlier makes the mean misleading, median is more representative\n\nage_median = df_titanic[\"Age\"].median()\ndf_titanic[\"Age\"] = df_titanic[\"Age\"].fillna(age_median)\n\n# 2. Fill categorical missing values (Embarked) with MODE\n# WHY MODE FOR CATEGORICAL?\n# - You cannot take \"average\" of categorical values (S, C, Q ports)\n# - Mode represents the most frequent/most likely value\n# - Statistically the best guess for a missing categorical value\n# - Minimizes introduction of new categories\n\nembarked_mode = df_titanic[\"Embarked\"].mode()[0]\ndf_titanic[\"Embarked\"] = df_titanic[\"Embarked\"].fillna(embarked_mode)\n\n# 3. WHY DROP CABIN COLUMN?\n# - 687 out of 891 values missing (77% missing!)\n# - Imputing would be mostly guessing\n# - Could introduce noise rather than signal\n# - Better to drop than to create artificial data\n# - Rule of thumb: Drop columns with >60-70% missing values\n\ndf_titanic = df_titanic.drop(columns=[\"Cabin\"])"
  },
  {
   "cell_type": "code",
   "source": "# Verify that we successfully handled all missing values\nprint(\"Missing values after handling:\")\ndf_titanic.isnull().sum()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOz1xVBENYnP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334591526,
     "user_tz": -360,
     "elapsed": 16,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "d80c1c21-c5a5-46b9-c16a-f6c9fa2b737d"
   },
   "id": "ZOz1xVBENYnP",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f595b49d",
   "metadata": {
    "id": "f595b49d"
   },
   "source": [
    "###Encoding Categorical Variables (Heart Dataset)\n",
    "\n",
    "Real-life analogy: **canteen token system**.\n",
    "\n",
    "- The canteen menu has items like *Tehari*, *Chowmein*, *Biriyani*.\n",
    "- The billing machine cannot understand these strings; it needs numeric codes.\n",
    "- However, assigning `Tehari = 1`, `Chowmein = 2`, `Biriyani = 3` does **not**\n",
    "  mean Biriyani is greater than Tehari. The numbers are **labels, not ranks**.\n",
    "\n",
    "In the Heart dataset, we will:\n",
    "- Use **Label Encoding** for binary categories like `Sex` and `ExerciseAngina`\n",
    "- Use **OneHot Encoding** for nominal categories like `ChestPainType`,\n",
    "  `RestingECG`, and `ST_Slope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ff623",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad0ff623",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334591736,
     "user_tz": -360,
     "elapsed": 201,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "1d9c5446-8702-4c5d-f5c4-d4f085076236"
   },
   "outputs": [],
   "source": "from sklearn.preprocessing import LabelEncoder\n\n# Load Heart dataset for working with categorical encoding\nheart_path = \"/content/sample_data/heart.csv\"  # change if needed\ndf_heart = pd.read_csv(heart_path)\n\n# Display the data and understand the structure\nprint(\"First 10 rows of Heart dataset:\")\ndisplay(df_heart.head(10))\n\n# Check data types to identify which columns need encoding\nprint(\"\\nColumn data types:\")\ndisplay(df_heart.dtypes)"
  },
  {
   "cell_type": "code",
   "source": "# Explore categorical features by creating bar charts\n# This helps us understand the distribution and identify which encoding method to use\ncategorical_cols = [\"Sex\", \"ChestPainType\", \"RestingECG\",\n                    \"ExerciseAngina\", \"ST_Slope\"]\nfor c in categorical_cols:\n  plt.figure(figsize=(5,4))\n  # Create bar chart showing value counts for each categorical column\n  df_heart[c].value_counts().plot(kind=\"bar\")\n  plt.title(f\"Value counts for {c}\")\n  plt.ylabel(\"Count\")\n  plt.tight_layout()\n  plt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "do8kflLTbd8D",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334593900,
     "user_tz": -360,
     "elapsed": 2159,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "471b9c57-8783-4226-f3a4-8511fde92287"
   },
   "id": "do8kflLTbd8D",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f0ab36",
   "metadata": {
    "id": "43f0ab36"
   },
   "outputs": [],
   "source": "# WHY LABEL ENCODING FOR BINARY COLUMNS:\n\n# Computers only understand numbers, not text categories like 'M'/'F' or 'Y'/'N'\n# WHY NOT JUST USE THE TEXT DIRECTLY?\n# - Most ML algorithms require numerical input\n# - Mathematical operations (addition, multiplication) need numbers\n# - Distance calculations (used in clustering, k-NN) require numerical values\n\n# WHY DIFFERENT ENCODING FOR BINARY vs MULTI-CATEGORY?\n\n# LABEL ENCODING for binary (2-value) columns is SAFE because:\n# - Sex: M=1, F=0 (or vice versa) - there are only 2 values\n# - ExerciseAngina: Y=1, N=0 - there are only 2 values\n# - No implicit ordering created: 1 is not \"better\" than 0\n# - Mathematical operations still make sense\n\n# WHY NOT ONE-HOT ENCODE BINARY COLUMNS?\n# - Would create 2 columns for Sex: Sex_M and Sex_F\n# - This is redundant (if Sex_M=0, we know Sex_F=1)\n# - Doubles number of columns unnecessarily\n# - Wastes memory and computational resources\n\nle = LabelEncoder()\ndf_heart[\"Sex\"] = le.fit_transform(df_heart[\"Sex\"])\ndf_heart[\"ExerciseAngina\"] = le.fit_transform(df_heart[\"ExerciseAngina\"])"
  },
  {
   "cell_type": "code",
   "source": "# View the data after label encoding to see the transformation\ndf_heart.head(10)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCBMPHcresh4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334594008,
     "user_tz": -360,
     "elapsed": 89,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "b39b0639-d701-45bd-861d-ab7cdcad9fd6"
   },
   "id": "UCBMPHcresh4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26c7d1",
   "metadata": {
    "id": "bc26c7d1"
   },
   "outputs": [],
   "source": "# WHY ONE-HOT ENCODING FOR MULTI-CATEGORY COLUMNS:\n\n# PROBLEM WITH LABEL ENCODING MULTI-CATEGORIES:\n# If we did: ATA=0, NAP=1, ASY=2, TA=3 for ChestPainType\n# - ML algorithm might think TA (3) > ASY (2) > NAP (1) > ATA (0)\n# - Creates artificial ordering that doesn't exist\n# - Distance algorithms would treat TA as \"farther\" from ATA than ASY\n# - This is WRONG - these are just different types, not ordered\n\n# ONE-HOT ENCODING SOLVES THIS BY:\n# Creating separate binary columns for each category:\n# - ChestPainType_ATA: [1,0,0,0] if ATA, [0,0,0,0] if not ATA\n# - ChestPainType_NAP: [0,1,0,0] if NAP, [0,0,0,0] if not NAP\n# - etc.\n\n# WHY THIS WORKS BETTER:\n# - No artificial ordering created\n# - Each category treated independently\n# - Algorithm can learn patterns for each category separately\n# - Distance calculations are meaningful (presence/absence of each category)\n\ncat_cols = [\"ChestPainType\", \"RestingECG\", \"ST_Slope\"]\n\n# pd.get_dummies automatically creates binary columns for each category\n# Example: ChestPainType becomes ChestPainType_ASY, ChestPainType_ATA, etc.\ndf_heart_encoded = pd.get_dummies(\n    df_heart,\n    columns = cat_cols,\n    dtype=int  # Convert True/False to 1/0 for better memory usage and ML compatibility\n)"
  },
  {
   "cell_type": "code",
   "source": "# View the dataset after one-hot encoding\n# Notice how each categorical column has been converted to multiple binary columns\ndf_heart_encoded.head(10)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwDyUKJpf2sY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334594364,
     "user_tz": -360,
     "elapsed": 341,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "42cf6978-fbe0-4d0d-d4c2-8eb05eac68b1"
   },
   "id": "HwDyUKJpf2sY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5652c5e2",
   "metadata": {
    "id": "5652c5e2"
   },
   "source": [
    "###Normalization and Scaling\n",
    "\n",
    "Real-life analogy: **comparing salary and height**.\n",
    "\n",
    "- Height might range from 150 to 190 cm.\n",
    "- Salary might range from 20,000 to 700,000.\n",
    "- If we feed these two features directly into a distance-based model,\n",
    "  salary will dominate the calculation.\n",
    "\n",
    "To fix this, we **scale** numeric features so they are on a comparable range.\n",
    "\n",
    "Common approaches:\n",
    "- **StandardScaler**: transforms features to have mean 0 and standard deviation 1\n",
    "- **MinMaxScaler**: rescales features to a fixed range, usually [0, 1]\n",
    "\n",
    "Always fit the scaler on the **training set only**, then transform both\n",
    "training and test sets using the same scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceabd17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ceabd17",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334595519,
     "user_tz": -360,
     "elapsed": 1152,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "f872c0fe-edd5-42d7-8ddc-a25d9ff422e3"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# WHY FEATURE SCALING IS CRUCIAL:\n\n# PROBLEM: FEATURES WITH DIFFERENT SCALES\n# Example in our dataset:\n# - Age: 28-77 years\n# - Cholesterol: 85-603 mg/dL  \n# - MaxHR: 60-202 bpm\n# - Oldpeak: 0-6.2\n\n# WHY THIS CAUSES PROBLEMS:\n# 1. ALGORITHMS USING DISTANCE (k-NN, SVM, Clustering):\n#    - Cholesterol (603) would dominate distance calculations\n#    - Age (28-77) would have almost no impact\n#    - Similar to comparing \"height in meters\" vs \"income in dollars\"\n#    - Income would completely dominate the distance metric\n\n# 2. GRADIENT DESCENT OPTIMIZATION:\n#    - Large features cause slow convergence\n#    - Optimizer gets \"stuck\" on large-scale features\n#    - Similar to climbing a mountain with uneven steps\n\n# 3. REGULARIZATION (L1, L2):\n#    - Penalizes large coefficient values\n#    - Unscaled features would be unfairly penalized\n#    - Cholesterol would get much larger penalty than Age\n\n# Split features and target BEFORE scaling (CRUCIAL!)\ntarget_col = \"HeartDisease\"\nX = df_heart_encoded.drop(columns=[target_col])\ny = df_heart_encoded[target_col]\n\n# WHY TRAIN-TEST SPLIT BEFORE SCALING:\n# 1. PREVENT DATA LEAKAGE:\n#    - If we scale before splitting, test data influences training scaling\n#    - Test set statistics \"leak\" into training process\n#    - Creates artificially optimistic results\n\n# 2. REALISTIC EVALUATION:\n#    - In real world, we don't know future data distribution\n#    - Scaling parameters must come from training data only\n#    - Test data is treated as completely \"unseen\"\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state=42)\n\n# STANDARD SCALING: Z-score normalization\n# Formula: (x - μ) / σ where μ=mean, σ=standard deviation\n# Result: mean=0, std=1 for all features\n# WHY USE THIS:\n# - Preserves original distribution shape\n# - Good for algorithms assuming normal distribution\n# - Handles outliers reasonably well\nscaler_sd = StandardScaler()\nX_train_std = scaler_sd.fit_transform(X_train)  # Learn parameters from training data\nX_test_std = scaler_sd.transform(X_test)       # Apply same transformation to test\n\n# MINMAX SCALING: Feature scaling to [0,1] range\n# Formula: (x - min) / (max - min)\n# Result: all values between 0 and 1\n# WHY USE THIS:\n# - Preserves zero values (important for sparse data)\n# - Good for algorithms requiring bounded inputs (neural networks)\n# - Maintains exact relationships between points\nscaler_mm = MinMaxScaler()\nX_train_mm = scaler_mm.fit_transform(X_train)\nX_test_mm = scaler_mm.transform(X_test)\n\n# Convert back to DataFrames for visualization\nprint(\"\\n--- Displaying Standard Scaled Data ---\")\nX_train_std_df = pd.DataFrame(X_train_std, columns = X_train.columns, index = X_train.index)\nX_test_std_df = pd.DataFrame(X_test_std, columns = X_test.columns, index = X_test.index)\nprint(\"\\nFirst 5 rows of X_train & X_test (Standard Scaled):\")\ndisplay(X_train_std_df.head())\ndisplay(X_test_std_df.head())\n\nprint(\"\\n--- Displaying MinMax Scaled Data ---\")\nX_train_mm_df = pd.DataFrame(X_train_mm, columns = X_train.columns, index = X_train.index)\nX_test_mm_df = pd.DataFrame(X_test_mm, columns = X_test.columns, index = X_test.index)\nprint(\"\\nFirst 5 rows of X_train & X_test (MinMax Scaled):\")\ndisplay(X_train_mm_df.head())\ndisplay(X_test_mm_df.head())"
  },
  {
   "cell_type": "markdown",
   "id": "a65ffb54",
   "metadata": {
    "id": "a65ffb54"
   },
   "source": [
    "---\n",
    "## Part 2: Outliers, Transformations, Pipelines, and Sanity Check\n",
    "\n",
    "In Part 2 we cover:\n",
    "- Outlier detection and handling\n",
    "- Feature transformation (polynomial, binning)\n",
    "- Domain-driven feature construction\n",
    "- Building a full preprocessing + modeling pipeline\n",
    "- Running a quick sanity check with a simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137fbc3",
   "metadata": {
    "id": "5137fbc3"
   },
   "source": [
    "###Outlier Detection and Handling\n",
    "\n",
    "Real-life analogy: **a student scoring 500 out of 100**.\n",
    "\n",
    "- Most students score between 60 and 90.\n",
    "- One student has a recorded score of 500.\n",
    "- This is likely a data entry error or an outlier.\n",
    "\n",
    "If we include this in the average, the class average becomes meaningless.\n",
    "Similarly, in ML, extreme values can distort models.\n",
    "\n",
    "We will:\n",
    "- Visualize outliers with a boxplot\n",
    "- Use the **IQR method** to detect outliers\n",
    "- Demonstrate removal, capping, and transformation approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57442b",
   "metadata": {
    "id": "bd57442b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334598906,
     "user_tz": -360,
     "elapsed": 572,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "outputId": "14f45b8d-6454-4ade-bc3d-62cc2defdac0"
   },
   "outputs": [],
   "source": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Visualize outliers using boxplots\n# Boxplots show the distribution and identify potential outliers\n# Points outside the \"whiskers\" are considered outliers\n\nnumeric_cols = [\"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=df_heart_encoded[numeric_cols])\nplt.title(\"Boxplots for selected numeric features\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c50715",
   "metadata": {
    "id": "93c50715",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334602156,
     "user_tz": -360,
     "elapsed": 9,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "f472dbf6-0386-42a4-d644-47b6a6c43281"
   },
   "outputs": [],
   "source": "# WHY IQR METHOD FOR OUTLIER DETECTION:\n\n# THE OUTLIER PROBLEM:\n# - Extreme values can dramatically skew statistical measures\n# - Mean becomes unrepresentative of \"typical\" values\n# - Example: Ages [25, 28, 30, 32, 35, 250] → Mean=66.7 (misleading!)\n# - ML algorithms can be overly influenced by outliers\n# - Distance-based algorithms (k-NN, clustering) get distorted\n\n# WHY NOT OTHER METHODS?\n\n# 1. Z-SCORE METHOD (MEAN ± 3σ):\n#    - Assumes normal distribution\n#    - Mean itself is affected by outliers\n#    - Not robust for skewed data\n#    - Example: Age=250 makes mean high, making 250 seem \"normal\"\n\n# 2. VISUAL INSPECTION:\n#    - Subjective and inconsistent\n#    - Doesn't scale to large datasets\n#    - No clear threshold\n\n# 3. DOMAIN KNOWLEDGE:\n#    - Requires medical expertise for each variable\n#    - May miss less obvious outliers\n#    - Time-consuming\n\n# WHY IQR METHOD IS PREFERRED:\n# 1. ROBUST: Uses median and quartiles, not mean\n# 2. DISTRIBUTION-AGNOSTIC: Works for any distribution shape\n# 3. SIMPLE & INTERPRETABLE: Clear mathematical definition\n# 4. WIDELY ADOPTED: Industry standard\n\n# IQR METHOD EXPLAINED:\n# IQR = Q3 (75th percentile) - Q1 (25th percentile)\n# - Represents the \"middle 50%\" of data\n# - Less affected by extreme values\n# - Outliers defined as: values below Q1-1.5*IQR or above Q3+1.5*IQR\n# - The 1.5 multiplier is a statistically-validated threshold\n\ncol = \"Cholesterol\"\nQ1 = df_heart_encoded[col].quantile(0.25)  # 25th percentile - below this is bottom 25%\nQ3 = df_heart_encoded[col].quantile(0.75)  # 75th percentile - above this is top 25%\nIQR = Q3 - Q1  # Range containing middle 50% of data\n\nlower = Q1 - 1.5*IQR  # Values below this are considered outliers\nupper = Q3 + 1.5*IQR  # Values above this are considered outliers\n\n# Identify outliers\noutliers = df_heart_encoded[(df_heart_encoded[col]<lower) | (df_heart_encoded[col]>upper)]\nprint(f\"Number of detected outliers in {col}: \", len(outliers))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918aaad",
   "metadata": {
    "id": "5918aaad",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764334604188,
     "user_tz": -360,
     "elapsed": 39,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "outputId": "22881fda-59ee-469a-f44d-b784563eeba6"
   },
   "outputs": [],
   "source": "# WHY DIFFERENT OUTLIER HANDLING STRATEGIES:\n\n# OUTLIERS ARE NOT ALWAYS ERRORS:\n# In medical data, outliers can be:\n# 1. DATA ENTRY ERRORS: Age=250, Cholesterol=5000 (should be removed/corrected)\n# 2. EXTREME BUT REAL: Cholesterol=400 in a patient with genetic disorder (valuable info)\n# 3. SPECIAL POPULATIONS: Elite athletes, patients with rare conditions\n# 4. MEASUREMENT ERRORS: Equipment malfunction, improper technique\n\n# THE DILEMMA: Remove outliers vs keep them?\n# WRONG APPROACH: Always remove all outliers\n# RIGHT APPROACH: Consider the medical context and choose appropriate strategy\n\n# STRATEGY 1: REMOVE OUTLIERS\n# WHEN TO USE:\n# - Clear data entry errors (impossible medical values)\n# - Very small percentage of outliers (<5%)\n# - Outliers don't represent medically meaningful cases\n# - Need clean, normally distributed data for certain algorithms\n\n# MEDICAL EXAMPLES FOR REMOVAL:\n# - Age=250 (impossible human age)\n# - Heart Rate=0 for living patient (impossible)\n# - Systolic BP < Diastolic BP (measurement error)\n\n# PROS OF REMOVAL:\n# - Clean, reliable dataset\n# - Reduces noise and distortion\n# - Improves performance of sensitive algorithms\n# - Easier statistical analysis\n\n# CONS OF REMOVAL:\n# - Loses potentially valuable extreme cases\n# - May bias results if outliers represent real subpopulations\n# - Reduces dataset size\n# - Might remove medically important edge cases\n\ndf_no_outliers = df_heart_encoded[(df_heart_encoded[col] >= lower) & (df_heart_encoded[col] <= upper)]\n\n# STRATEGY 2: CAP OUTLIERS (WINSORIZATION)\n# WHEN TO USE:\n# - Outliers are extreme but possibly real\n# - Want to preserve data points but reduce their impact\n# - Suspected measurement error but want to keep the case\n# - Maintaining dataset size is important\n\n# MEDICAL EXAMPLES FOR CAPPING:\n# - Cholesterol=600 → cap at upper normal limit (400)\n# - Blood Pressure=200/120 → cap at reasonable high value\n# - Heart Rate=200 → cap at physiological maximum\n\n# PROS OF CAPPING:\n# - Preserves all data points\n# - Reduces extreme influence while keeping information\n# - More robust to occasional measurement errors\n# - Maintains statistical power\n\n# CONS OF CAPPING:\n# - Creates artificial values that don't exist in reality\n# - May distort relationships between variables\n# - Can mask important extreme cases\n# - Less interpretable than removal\n\ndf_capped = df_heart_encoded.copy()\ndf_capped[col] = df_capped[col].clip(lower, upper)\n\n# STRATEGY 3: LOG TRANSFORMATION\n# WHEN TO USE:\n# - Data is highly skewed (right-skewed distributions)\n# - Want to preserve relative ordering while reducing scale\n# - Biological processes that work on multiplicative scales\n# - When absolute differences matter less than relative differences\n\n# MEDICAL EXAMPLES FOR LOG TRANSFORM:\n# - Cholesterol: 200 vs 400 is similar to 400 vs 800 (doubling effect)\n# - White blood cell count: Small numbers vs very large counts\n# - Drug concentrations: Exponential decay/accumulation patterns\n# - Enzyme levels: Orders of magnitude differences\n\n# WHY LOG WORKS WELL:\n# - Compresses large values more than small values\n# - Reduces right-skewness toward normality\n# - Makes multiplicative relationships additive\n# - Handles zero values (with +1 adjustment)\n\n# PROS OF LOG TRANSFORM:\n# - Preserves all data and ordering\n# - Handles wide range of values effectively\n# - Often makes distributions more normal\n# - Good for linear models and distance-based algorithms\n\n# CONS OF LOG TRANSFORM:\n# - Changes scale of interpretation\n# - Cannot handle zero/negative values (without adjustment)\n# - Makes back-transformations complex\n# - May be harder to explain to clinicians\n\ndf_log = df_heart_encoded.copy()\ndf_log[col + \"_log\"] = np.log(df_log[col] + 1)  # +1 to avoid log(0)\n\n# STRATEGY SELECTION GUIDE:\n# REMOVAL: When outliers are clearly erroneous and rare (<5% of data)\n# CAPPING: When outliers are extreme but represent real cases, want to preserve data\n# LOG TRANSFORM: When data is highly skewed with wide range of values\n\nprint(\"Dataset after removing outliers:\", len(df_no_outliers))\nprint(\"Dataset after capping outliers:\", len(df_capped))\nprint(\"Dataset with log transformation:\", len(df_log))\n\n# CLINICAL CONTEXT FOR CHOLESTEROL OUTLIERS:\n# - Cholesterol < 100: Possible measurement error or malnutrition (keep but investigate)\n# - Cholesterol 100-400: Normal to high but medically plausible\n# - Cholesterol 400-600: Very high but possible in genetic disorders\n# - Cholesterol > 600: Likely measurement error, consider removal/capping"
  },
  {
   "cell_type": "markdown",
   "id": "f593e97f",
   "metadata": {
    "id": "f593e97f"
   },
   "source": [
    "###Feature Transformation and Domain-Driven Features\n",
    "\n",
    "Real-life analogy: **electricity bill categories**.\n",
    "\n",
    "- Monthly bills: 2400, 2600, 2800, 3000, 9000, 2700.\n",
    "- As raw numbers, it is hard to see a pattern.\n",
    "- If we convert them to categories (LOW, MEDIUM, HIGH),\n",
    "  it becomes easier to understand behavior.\n",
    "\n",
    "In this section we:\n",
    "- Create **polynomial features** to capture non-linear relationships\n",
    "- Use **binning** to create groups from continuous variables\n",
    "- Create **domain-driven features** such as blood pressure risk levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c356a6",
   "metadata": {
    "id": "a2c356a6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764337097545,
     "user_tz": -360,
     "elapsed": 22,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1301085c-17b2-42fa-e6a8-78a01c2a1d65"
   },
   "outputs": [],
   "source": "from sklearn.preprocessing import PolynomialFeatures\n\n# WHY POLYNOMIAL FEATURES:\n\n# THE LIMITATION OF LINEAR MODELS:\n# Linear models assume: y = β₀ + β₁x₁ + β₂x₂ + ... \n# This means they can only capture straight-line relationships\n# Example: Heart disease risk increases linearly with age\n# Real relationships are often non-linear!\n\n# REAL-WORLD NON-LINEAR EXAMPLES:\n# 1. Age and Heart Disease: Risk increases slowly, then rapidly after 60\n# 2. Blood Pressure: Both very low and very high are dangerous (U-shaped curve)\n# 3. Exercise: More exercise is good, but excessive exercise can be harmful\n# 4. Medication: Right dose helps, too little or too much is harmful\n\n# HOW POLYNOMIAL FEATURES HELP:\n# Instead of just: Risk = β₀ + β₁×Age + β₂×MaxHR\n# We get: Risk = β₀ + β₁×Age + β₂×MaxHR + β₃×Age² + β₄×Age×MaxHR + β₅×MaxHR²\n\n# WHY DEGREE=2 (NOT HIGHER)?\n# 1. DEGREE=1: Original linear model (missing non-linear patterns)\n# 2. DEGREE=2: Captures curves and interactions (sweet spot for most problems)\n# 3. DEGREE=3+: Higher order polynomials\n#    - Pros: Can capture more complex patterns\n#    - Cons: Risk of overfitting, harder to interpret\n#    - Rule: Start with degree=2, only go higher if needed\n\n# WHAT POLYNOMIAL FEATURES CREATE:\n# For Age and MaxHR with degree=2:\n# - Age: Original feature\n# - MaxHR: Original feature  \n# - Age²: Age squared (captures accelerating/decelerating effects)\n# - Age×MaxHR: Interaction term (captures combined effects)\n# - MaxHR²: MaxHR squared\n\n# WHY INTERACTION TERMS MATTER:\n# Medical example: Age×MaxHR interaction\n# - High MaxHR might be good for young people (good fitness)\n# - High MaxHR might be bad for old people (heart strain)\n# - The effect of MaxHR depends on Age\n# - Interaction term captures this relationship\n\n# POTENTIAL MEDICAL INTERPRETATIONS:\n# 1. Age²: Risk increases faster as you get older\n# 2. MaxHR²: Very high or very low heart rate is problematic\n# 3. Age×MaxHR: Effect of heart rate depends on age\n\npoly_cols = [\"Age\", \"MaxHR\"]\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n# Transform features to create polynomial combinations\npoly_features = poly.fit_transform(df_heart_encoded[poly_cols])\npoly_feature_names = poly.get_feature_names_out(poly_cols)\n\nprint(\"Created polynomial features:\", poly_feature_names)\nprint(\"Shape of transformed data:\", poly_features.shape)\n\n# INTERPRETING THE RESULTS:\n# Original: 2 features (Age, MaxHR)\n# After polynomial: 5 features (Age, MaxHR, Age², Age×MaxHR, MaxHR²)\n# This gives the model more flexibility to capture non-linear relationships"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bad221",
   "metadata": {
    "id": "09bad221",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764337815217,
     "user_tz": -360,
     "elapsed": 112,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "22ff1441-5f68-4c7a-c47c-dcaf9d446003"
   },
   "outputs": [],
   "source": "# WHY BINNING (DISCRETIZATION):\n\n# THE PROBLEM WITH CONTINUOUS VARIABLES:\n# Models struggle with very precise continuous values:\n# - Age 28.1 vs 28.2 vs 28.3 are treated as very different\n# - But medically, these ages are essentially the same risk level\n# - Creates unnecessary complexity and potential overfitting\n\n# REAL-WORLD BINNING EXAMPLES:\n# 1. Temperature: 98.6°F, 99.1°F, 101.2°F → \"Normal\", \"Fever\", \"High Fever\"\n# 2. Credit Score: 680, 724, 761 → \"Fair\", \"Good\", \"Excellent\"\n# 3. Blood Pressure: 118/75, 135/88, 162/95 → \"Normal\", \"Elevated\", \"High\"\n\n# WHY BINNING HELPS:\n\n# 1. SIMPLIFICATION:\n#    - Reduces noise from minor variations\n#    - Groups similar values together\n#    - Makes patterns more obvious to models\n\n# 2. NON-LINEARITY CAPTURE:\n#    - Age 25 vs 35: small difference in heart disease risk\n#    - Age 65 vs 75: large difference in heart disease risk\n#    - Binning captures these non-linear relationships\n#    - Example: \"Young\" (0-30), \"Middle\" (30-50), \"Old\" (70-100)\n\n# 3. MISSING VALUE HANDLING:\n#    - Can create \"Unknown\" category for missing data\n#    - Better than imputation sometimes\n#    - Preserves information about missingness\n\n# 4. INTERPRETABILITY:\n#    - \"People over 70 have 3x higher risk\" vs \"Age has coefficient 0.023\"\n#    - Easier for doctors and stakeholders to understand\n#    - More intuitive decision-making\n\n# 5. OUTLIER HANDLING:\n#    - Extreme values get grouped into bins\n#    - Reduces impact of individual outliers\n#    - More robust to data errors\n\n# POTENTIAL DOWNSIDES (WHEN NOT TO BIN):\n# 1. INFORMATION LOSS: Precise values become categories\n# 2. ARBITRARY BOUNDARIES: Why 30 vs 31, not 29 vs 30?\n# 3. MODEL DEPENDENCY: Some models (like trees) handle continuous data well\n# 4. DOMAIN KNOWLEDGE: Need medical expertise to set meaningful bin boundaries\n\n# MEDICAL EXAMPLE: AGE BINNING\n# Why these boundaries (0-30, 30-50, 50-70, 70-100)?\n# - 0-30: Young adult, lowest heart disease risk\n# - 30-50: Middle age, risk slowly increasing\n# - 50-70: Pre-senior, risk rapidly increasing  \n# - 70-100: Senior, highest heart disease risk\n# - These align with medical guidelines and risk assessment\n\ndf_heart_encoded[\"Age_bin\"] = pd.cut(\n    df_heart_encoded[\"Age\"],\n    bins=[0, 30, 50, 70, 100],  # Age boundaries: 0-30, 30-50, 50-70, 70-100\n    labels=[\"Young\", \"Middle\", \"Middle Old\", \"Old\"]  # Clinically meaningful categories\n)\n\n# Show the transformation from continuous to categorical\nprint(\"Age binning results:\")\nprint(df_heart_encoded[[\"Age\", \"Age_bin\"]].head(50))\n\n# INTERPRETATION:\n# - Age 40 becomes \"Middle\" (medically meaningful category)\n# - Age 54 becomes \"Middle Old\" (higher risk category)\n# - Age 72 becomes \"Old\" (highest risk category)\n# - Model can now learn category-based risk patterns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704fd07",
   "metadata": {
    "id": "9704fd07",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764338587338,
     "user_tz": -360,
     "elapsed": 141,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "outputId": "b9c8c223-38a8-4c61-ad35-e9968e3de7c2"
   },
   "outputs": [],
   "source": "# WHY DOMAIN-DRIVEN FEATURE ENGINEERING:\n\n# THE LIMITATION OF GENERIC FEATURES:\n# Raw medical values don't tell the complete clinical story:\n# - BP=140 mmHg: Is this good or bad?\n# - Cholesterol=210 mg/dL: Is this concerning?\n# - Oldpeak=1.5: What does this mean clinically?\n# - ML models need context to understand significance\n\n# WHY DOMAIN KNOWLEDGE TRANSFORMATIONS ARE POWERFUL:\n\n# 1. CLINICAL CONTEXT:\n#    - Medical guidelines have established thresholds\n#    - Doctors think in terms of \"risk categories\", not raw numbers\n#    - Features become meaningful to both clinicians and models\n\n# 2. NON-LINEAR RISK PATTERNS:\n#    - Risk doesn't increase linearly with most medical measurements\n#    - BP: Both 80 and 180 are problematic (U-shaped risk curve)\n#    - Raw linear features miss this pattern completely\n\n# 3. INTERPRETABILITY:\n#    - \"High BP Risk\" vs \"BP=142 mmHg coefficient = 0.023\"\n#    - Doctors can understand and validate the model's logic\n#    - Easier to explain to patients and stakeholders\n\n# 4. REDUCED DATA REQUIREMENTS:\n#    - Domain features capture more information per feature\n#    - Model can learn with less data\n#    - More robust to small datasets\n\n# 5. SIMPLIFIED RELATIONSHIPS:\n#    - Complex medical guidelines become simple categorical rules\n#    - Reduces model complexity needed to capture patterns\n#    - Less risk of overfitting\n\n# MEDICAL AUTHORITY BEHIND THESE THRESHOLDS:\n# American Heart Association (AHA) and European Society of Cardiology (ESC) guidelines\n\ndef bp_risk(bp):\n    \"\"\"Classify blood pressure into risk categories based on medical standards\"\"\"\n    if bp < 120:\n        return \"Normal\"      # <120/80 = Optimal blood pressure\n    elif bp < 140:\n        return \"Elevated\"    # 120-139 = Elevated blood pressure\n    else:\n        return \"High\"        # ≥140 = High blood pressure (Stage 1+)\n    # Medical basis: AHA/ACC 2017 Blood Pressure Guidelines\n    # Risk increases stepwise, not linearly\n\ndef oldpeak_risk(op):\n    \"\"\"Classify ST depression (Oldpeak) into stress risk levels\"\"\"\n    if op == 0:\n        return \"No Stress\"       # 0 = Normal ST segment response\n    elif op < 2:\n        return \"Moderate Stress\" # 1-2 = Mild ST depression\n    else:\n        return \"High Stress\"     # >2 = Significant ST depression\n    # Medical basis: Exercise stress test interpretation guidelines\n    # ST depression indicates myocardial ischemia (reduced blood flow)\n\n# WHY THESE SPECIFIC TRANSFORMATIONS:\n\n# BLOOD PRESSURE CATEGORIES:\n# Medical rationale:\n# - <120: Minimal cardiovascular risk\n# - 120-139: 2x increased risk compared to normal\n# - ≥140: 4x increased risk compared to normal\n# - These are evidence-based thresholds from large clinical trials\n\n# OLDPEAK (ST DEPRESSION) CATEGORIES:\n# Medical rationale:\n# - 0: No evidence of exercise-induced ischemia\n# - <2: Mild ischemia, moderate risk\n# - ≥2: Significant ischemia, high risk\n# - These predict coronary artery disease severity\n\n# APPLY DOMAIN KNOWLEDGE TO CREATE CLINICALLY MEANINGFUL FEATURES\ndf_heart_encoded[\"BP_Risk\"] = df_heart_encoded[\"RestingBP\"].apply(bp_risk)\ndf_heart_encoded[\"Oldpeak_Risk\"] = df_heart_encoded[\"Oldpeak\"].apply(oldpeak_risk)\n\n# Display the transformation from raw values to clinical categories\nprint(\"Domain-driven risk categories:\")\nprint(df_heart_encoded[[\"RestingBP\", \"BP_Risk\", \"Oldpeak\", \"Oldpeak_Risk\"]])\n\n# EXPECTED IMPACT ON MODEL PERFORMANCE:\n# 1. Better interpretability for clinicians\n# 2. More accurate risk assessment\n# 3. Reduced overfitting (categorical vs continuous)\n# 4. Incorporates medical expertise into ML\n# 5. Features align with clinical decision-making"
  },
  {
   "cell_type": "markdown",
   "id": "a1a0091e",
   "metadata": {
    "id": "a1a0091e"
   },
   "source": [
    "###Putting It All Together in a Preprocessing Pipeline\n",
    "\n",
    "Real-life analogy: **garments factory assembly line**.\n",
    "\n",
    "- Cutting → Stitching → Printing → Ironing → Packaging.\n",
    "- Each step must happen in the correct order.\n",
    "- A pipeline in ML ensures preprocessing and modeling steps run\n",
    "  in a fixed sequence without mixing training and test data.\n",
    "\n",
    "We will:\n",
    "- Build separate pipelines for numeric and categorical features\n",
    "- Combine them using `ColumnTransformer`\n",
    "- Attach a simple model (Logistic Regression) at the end\n",
    "  to create a full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676c5f0",
   "metadata": {
    "id": "f676c5f0"
   },
   "outputs": [],
   "source": "from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# WHY WE NEED ML PIPELINES:\n\n# THE PROBLEM WITHOUT PIPELINES:\n# Manual preprocessing is error-prone and inconsistent:\n# 1. DATA LEAKAGE: Accidentally using test data to influence preprocessing\n# 2. INCONSISTENT PROCESSING: Forgetting to apply same transformations to test data\n# 3. CODE MAINTENANCE: Hard to track and reproduce preprocessing steps\n# 4. PRODUCTION DEPLOYMENT: Complex to deploy with multiple steps\n\n# REAL-WORLD SCENARIO WHERE THINGS GO WRONG:\n# 1. Manual approach: scale data → split → train → model\n# 2. Test data: forget to scale → prediction fails\n# 3. Or: scale test data separately → different scaling parameters → wrong predictions\n# 4. Or: scale after splitting → test data influences training → data leakage\n\n# WHY PIPELINES SOLVE THESE PROBLEMS:\n# 1. AUTOMATIC PROPER SEQUENCE: Ensures operations happen in correct order\n# 2. NO DATA LEAKAGE: Fit only on training data, transform both train and test\n# 3. REPRODUCIBILITY: Same preprocessing applied consistently\n# 4. PRODUCTION READY: Single object that handles everything\n# 5. EASY MAINTENANCE: All preprocessing steps in one place\n\n# Define column types for different preprocessing\nnum_features = [\"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]\ncat_features = [\"Sex\", \"ExerciseAngina\", \"ChestPainType\", \"RestingECG\", \"ST_Slope\"]\n\n# NUMERIC PIPELINE: WHY SEPARATE PIPELINES?\n# Different preprocessing for different data types:\n# - Numeric: Scaling (StandardScaler, MinMaxScaler, etc.)\n# - Categorical: Encoding (OneHotEncoder, LabelEncoder, etc.)\n# - Text: Vectorization (TfidfVectorizer, CountVectorizer, etc.)\n# - Custom: Domain-specific transformations\n\nnum_pipeline = Pipeline([\n    (\"scaler\", StandardScaler())  # Each step has a name for reference\n])\n\n# CATEGORICAL PIPELINE: WHY ONE-HOT WITH DROP_FIRST?\ncat_pipeline = Pipeline([\n    (\"ohe\", OneHotEncoder(drop=\"first\"))  # Remove first category to avoid multicollinearity\n])\n\n# WHY DROP_FIRST IN ONE-HOT ENCODING?\n# PROBLEM: Multicollinearity in linear models\n# - If we have all categories: Cat_A, Cat_B, Cat_C\n# - When Cat_A=0 and Cat_B=0, we know Cat_C=1\n# - This creates perfect correlation between variables\n# - Makes matrix inversion unstable for linear models\n\n# SOLUTION: Drop one category\n# - Keep Cat_A, Cat_B (drop Cat_C)\n# - When Cat_A=0 and Cat_B=0, model knows it's Cat_C\n# - Reduces redundancy while preserving all information\n# - Better numerical stability and interpretability\n\n# COMBINE PIPELINES: WHY COLUMNTRANSFORMER?\n# ColumnTransformer applies different transformations to different columns:\n# - Numeric columns → get StandardScaler\n# - Categorical columns → get OneHotEncoder\n# - Can mix 3+ transformation types\n# - Maintains column order and relationships\n\npreprocess = ColumnTransformer([\n    (\"num\", num_pipeline, num_features),  # Apply numeric pipeline to numeric columns\n    (\"cat\", cat_pipeline, cat_features)   # Apply categorical pipeline to categorical columns\n])\n\n# FULL PIPELINE: WHY INCLUDE MODEL IN PIPELINE?\n# BENEFITS:\n# 1. ENCAPSULATION: One object handles everything\n# 2. PROPER SCORING: Cross-validation works correctly\n# 3. HYPERPARAMETER TUNING: Can tune preprocessing and model together\n# 4. PREDICTION CONSISTENCY: Same preprocessing applied to new data\n# 5. PRODUCTION DEPLOYMENT: Single object to save/load\n\nclf = Pipeline([\n   (\"prep\", preprocess),                     # Preprocessing step\n   (\"model\", LogisticRegression(max_iter=1000))  # Machine learning model\n])"
  },
  {
   "cell_type": "markdown",
   "id": "d26dc212",
   "metadata": {
    "id": "d26dc212"
   },
   "source": [
    "###Quick Sanity Check with a Simple Model\n",
    "\n",
    "Real-life analogy: **test-driving a car before buying it**.\n",
    "\n",
    "- You do not buy a car based only on brochures.\n",
    "- You take it for a test drive to see if everything works as expected.\n",
    "\n",
    "Similarly, before building complex models, we run a simple model (e.g.,\n",
    "Logistic Regression) with our preprocessing pipeline to ensure:\n",
    "- The pipeline runs without errors\n",
    "- There is no data leakage\n",
    "- The accuracy is reasonable for a first attempt\n",
    "\n",
    "This is not the final model, just a **sanity check**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29684f3f",
   "metadata": {
    "id": "29684f3f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764341014838,
     "user_tz": -360,
     "elapsed": 172,
     "user": {
      "displayName": "Md. Sabbir Ahmed",
      "userId": "04742604537752069967"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1a7ddd36-811e-4334-c278-aefac39227fd"
   },
   "outputs": [],
   "source": "# WHY THIS \"SANITY CHECK\" IS CRUCIAL:\n\n# THE PROBLEM: BUILDING COMPLEX MODELS ON BROKEN DATA\n# Many data scientists make this mistake:\n# 1. Spend weeks on complex models (Deep Learning, XGBoost, etc.)\n# 2. Get poor results\n# 3. Waste time tuning hyperparameters\n# 4. Finally realize the preprocessing was wrong\n# 5. Have to redo everything from scratch\n\n# WHY START WITH A SIMPLE MODEL:\n# 1. QUICK VALIDATION: Confirms data pipeline works in <5 minutes\n# 2. BASELINE PERFORMANCE: Gives a reference point for improvement\n# 3. DEBUGGING: Easy to identify if problems are in data vs model\n# 4. EFFICIENCY: Better to find data problems early than late\n# 5. STAKEHOLDER COMMUNICATION: Can show \"something works\" quickly\n\n# WHY LOGISTIC REGRESSION FOR SANITY CHECK?\n# 1. SIMPLE AND INTERPRETABLE: Easy to understand what it's learning\n# 2. FAST TO TRAIN: Takes seconds, not hours\n# 3. WORKS WITH PREPROCESSED DATA: Good test of preprocessing pipeline\n# 4. ESTABLISHED BASELINE: Known performance characteristics\n# 5. ROBUST: Less likely to have implementation bugs\n\n# WHY TEST WITH ORIGINAL (UNPROCESSED) DATA?\n# THIS IS THE REAL TEST OF OUR PIPELINE!\n\n# Manual preprocessing test: \n# 1. Preprocess data manually → 84% accuracy\n# 2. Pipeline on preprocessed data → 84% accuracy \n# 3. BUT: Pipeline on raw data ???\n\n# If pipeline on raw data gives different results:\n# → Pipeline is doing something wrong\n# → Manual preprocessing had errors\n# → Data leakage in manual approach\n\n# If pipeline on raw data gives similar results:\n# → Pipeline is working correctly\n# → Manual preprocessing was correct\n# → Ready for more complex models\n\n# USE RAW DATA (not preprocessed) to test the complete pipeline\n# This simulates real-world scenarios where we get raw, unprocessed data\n\nX = df_heart.drop(columns=[target_col])  # Raw features, no preprocessing\ny = df_heart[target_col]                 # Target variable\n\n# Train-test split the same way (random_state=42 ensures consistency)\nX_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(\n    X, y, test_size = 0.25, random_state = 42\n)\n\n# FIT THE COMPLETE PIPELINE\n# This automatically performs:\n# 1. Categorical encoding (OneHotEncoder)\n# 2. Numeric scaling (StandardScaler)  \n# 3. Model training (LogisticRegression)\n# 4. All with proper train/test separation (no data leakage!)\n\nclf.fit(X_train_pipe, y_train_pipe)\n\n# PREDICT AND EVALUATE\n# The pipeline automatically:\n# 1. Applies same preprocessing to test data\n# 2. Uses learned encoders/scalers from training data\n# 3. Makes predictions with trained model\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred_pipe = clf.predict(X_test_pipe)\nacc = accuracy_score(y_test_pipe, y_pred_pipe)\nprint(\"Logistic Regression with preprocessing pipeline accuracy: \", acc)\n\n# INTERPRETING THE RESULT:\n# If accuracy ≈ 84% (similar to manual approach):\n# → SUCCESS! Pipeline is working correctly\n# → Ready to try more complex models\n# → Can trust the preprocessing pipeline\n\n# If accuracy is very different:\n# → PROBLEM! Something is wrong\n# → Need to debug the pipeline\n# → Check encoding, scaling, train/test split\n\n# This 84.3% accuracy validates our entire data preprocessing approach!"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10r1O9OBfKIGRGsuNXcN-c2K8SZL7Lp4H",
     "timestamp": 1764562080942
    },
    {
     "file_id": "1l2gC02mIv0Eh3UceBLGmpk7ey9Wj8iuf",
     "timestamp": 1763649301660
    },
    {
     "file_id": "15S0B6jAZMEVY3hHqeq6fZfeC2h1BM-JJ",
     "timestamp": 1763638524003
    }
   ]
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}