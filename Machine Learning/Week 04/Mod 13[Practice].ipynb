{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4e4437",
   "metadata": {
    "id": "1b4e4437"
   },
   "source": [
    "# Module 13 - Practice Notebook\n",
    "This notebook includes TODO markers **inside code cells** so students complete the missing parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wemljitaz5s",
   "source": "### Welcome to Module 13 Practice!\n\nThis notebook contains hands-on exercises to help you practice Multiple Linear Regression and Polynomial Regression concepts.\n\n**What you'll practice:**\n1. Working with a real insurance dataset\n2. Handling categorical features with OneHotEncoder\n3. Building pipelines for preprocessing\n4. Comparing different polynomial degrees\n5. Understanding overfitting through visualization\n\n**How to use this notebook:**\n- Look for `# TODO:` comments in code cells\n- Fill in the missing code as instructed\n- Run each cell to verify your solution\n- Read the explanations to understand each step\n\n**Tips for beginners:**\n- Take your time to understand each concept\n- Don't worry about making mistakes - that's how we learn!\n- Refer back to Mod 13.ipynb for examples\n- Ask questions if anything is unclear\n\nLet's get started!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343c98d",
   "metadata": {
    "id": "6343c98d"
   },
   "outputs": [],
   "source": "#Import all required libraries\nimport numpy as np\nimport pandas as pd\n\n# Scikit-learn imports for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nimport matplotlib.pyplot as plt\n\n# What each library does:\n# - numpy: For numerical operations and arrays\n# - pandas: For data manipulation and working with DataFrames\n# - train_test_split: To split data into training and testing sets\n# - LinearRegression: Our main regression model\n# - OneHotEncoder: Converts categorical variables to numeric format\n# - PolynomialFeatures: Creates polynomial terms for non-linear relationships\n# - ColumnTransformer: Applies different transformations to different columns\n# - Pipeline: Chains multiple steps together for cleaner code\n# - Metrics: Functions to evaluate model performance\n# - matplotlib: For creating visualizations and plots"
  },
  {
   "cell_type": "markdown",
   "id": "749386bb",
   "metadata": {
    "id": "749386bb"
   },
   "source": [
    "## Load Insurance Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cu9nzsryra",
   "source": "### About the Insurance Dataset\n\nThis dataset contains information about individuals and their insurance charges. It's a great example for regression because:\n\n**Features include:**\n- `age`: Age of the primary beneficiary\n- `sex`: Gender (male/female) - categorical\n- `bmi`: Body mass index (kg/m²)\n- `children`: Number of children covered\n- `smoker`: Whether the person smokes (yes/no) - categorical\n- `region`: Residential region in US (northeast, southeast, southwest, northwest) - categorical\n\n**Target:**\n- `charges`: Individual medical costs billed by health insurance\n\n**Why this dataset is interesting:**\n- Mix of numeric and categorical features\n- Likely non-linear relationships (e.g., age effects might not be linear)\n- Clear interactions (smoking probably amplifies age effects)\n- Real-world relevance for insurance companies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e52e39",
   "metadata": {
    "id": "74e52e39"
   },
   "outputs": [],
   "source": [
    "# Load the insurance dataset\n",
    "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "insurance = pd.read_csv(url)\n",
    "\n",
    "# TODO: Display first 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e096a2",
   "metadata": {
    "id": "c9e096a2"
   },
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tvvtaftegbd",
   "source": "### Understanding Dataset Information\n\nExploring your dataset is a crucial first step in any machine learning project. This helps you:\n\n**1. Understand your data:**\n- How many samples (rows) do you have?\n- What are the features (columns)?\n- Are there missing values?\n\n**2. Identify data types:**\n- Numeric features (integers, floats)\n- Categorical features (objects, strings)\n- This determines which preprocessing steps are needed\n\n**3. Get basic statistics:**\n- Mean, median, standard deviation for numeric features\n- Value counts for categorical features\n- Range of values to spot potential outliers\n\n**4. Plan your modeling approach:**\n- Which features might be predictive?\n- Do you need to create new features?\n- Should you scale or normalize anything?\n\nBest practice: Always explore your data before modeling!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f1a61",
   "metadata": {
    "id": "c53f1a61"
   },
   "outputs": [],
   "source": [
    "# TODO: Print dataset info\n",
    "\n",
    "\n",
    "# TODO: Print descriptive statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af598b4a",
   "metadata": {
    "id": "af598b4a"
   },
   "source": [
    "## Define Features and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "znkroca04dd",
   "source": "### Preparing Features and Target\n\nIn supervised machine learning, we need to clearly separate:\n\n**Features (X)**: The input variables we use to make predictions\n- Also called predictors, independent variables\n- Everything except what we want to predict\n- Must be in the right format for the model\n\n**Target (y)**: What we want to predict\n- Also called response, dependent variable, label\n- Single column we're trying to estimate\n- Must match the number of rows in X\n\n**Why this separation matters:**\n- Models learn patterns from X to predict y\n- During training, both X and y are provided\n- During prediction, only X is provided\n- Clear separation prevents data leakage\n\n**Common mistakes to avoid:**\n- Accidentally including target in features\n- Wrong shapes (X should be 2D, y should be 1D)\n- Mismatched number of samples between X and y",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d722d9",
   "metadata": {
    "id": "a1d722d9"
   },
   "outputs": [],
   "source": [
    "# TODO: Set numeric and categorical feature lists\n",
    "\n",
    "\n",
    "# TODO: Define target column\n",
    "\n",
    "\n",
    "# TODO: Create X and y\n",
    "\n",
    "\n",
    "# TODO: Display X head\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051934b",
   "metadata": {
    "id": "c051934b"
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hz00mq3lmuf",
   "source": "### Why Train-Test Split?\n\nSplitting data is fundamental to machine learning because:\n\n**Training Set** (typically 70-80% of data):\n- Used to teach/train the model\n- Model learns patterns from this data\n- Model parameters are adjusted based on this data\n\n**Test Set** (typically 20-30% of data):\n- Used only for evaluation\n- Model never sees this data during training\n- Provides unbiased estimate of real-world performance\n\n**Why not use all data for training?**\n- Would lead to overfitting (model memorizes data)\n- No way to know if model generalizes to new data\n- Inflated performance metrics\n- Poor real-world performance\n\n**The key principle:**\nTest set simulates \"future\" data the model will encounter in production.\n\n**Important:**\n- Always split BEFORE any preprocessing\n- Use same random_state for reproducibility\n- Never look at test data during model development",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c7c20",
   "metadata": {
    "id": "d58c7c20"
   },
   "outputs": [],
   "source": [
    "# Perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff999d7a",
   "metadata": {
    "id": "ff999d7a"
   },
   "source": [
    "## Multiple Linear Regression Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oa2jczz20l",
   "source": "### Handling Categorical Features\n\nMost machine learning models require numeric input, but real-world data often contains categorical variables (text labels). \n\n**Common categorical variables:**\n- Gender: male, female\n- Yes/No: smoker, non-smoker\n- Categories: region, product type\n- Ordinal: education level, satisfaction rating\n\n**One-Hot Encoding:**\n- Converts each category into a binary (0/1) column\n- Example: Region becomes region_northeast, region_southeast, etc.\n- Only one of these columns will be 1 for each row\n- `drop=\"first\"` removes one category to avoid multicollinearity\n\n**Why not label encoding (0, 1, 2)?**\n- Implies ordinal relationship that may not exist\n- Models might assume 2 > 1 > 0 has meaning\n- One-hot is safer for nominal categories\n\n**ColumnTransformer:**\n- Applies different preprocessing to different columns\n- Passes through numeric features unchanged\n- Applies OneHotEncoder to categorical features\n- Keeps track of which columns get which treatment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64762c0",
   "metadata": {
    "id": "d64762c0"
   },
   "outputs": [],
   "source": [
    "# Build preprocessing transformer\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# TODO: Build full pipeline with LinearRegression\n",
    "\n",
    "\n",
    "# TODO: Fit the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda30852",
   "metadata": {
    "id": "cda30852"
   },
   "source": [
    "## Regression Performance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glknsunbpe6",
   "source": "### Creating Evaluation Functions\n\nCreating reusable functions for evaluation makes your code:\n1. **Cleaner**: Don't repeat the same evaluation code\n2. **Consistent**: Same metrics calculated everywhere\n3. **Error-free**: Less chance of mistakes\n4. **Efficient**: Write once, use many times\n\n**Key metrics for regression:**\n- **MAE**: Mean Absolute Error (interpretable in target units)\n- **RMSE**: Root Mean Squared Error (punishes large errors more)\n- **R²**: R-squared (proportion of variance explained, 0-1)\n\n**What makes a good evaluation function:**\n- Clear, descriptive labels\n- Shows multiple metrics\n- Handles both train and test sets\n- Easy to compare different models\n\n**Best practices:**\n- Always evaluate both train AND test sets\n- Look for large gaps (sign of overfitting)\n- Consider the context when interpreting metrics\n- MAE is most interpretable for stakeholders",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694ba41",
   "metadata": {
    "id": "2694ba41"
   },
   "outputs": [],
   "source": [
    "# TODO: Complete function to print performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b156f49",
   "metadata": {
    "id": "1b156f49"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3fdd7",
   "metadata": {
    "id": "9dc3fdd7"
   },
   "outputs": [],
   "source": [
    "# Predict for train and test sets\n",
    "y_train_pred = mlr_model.predict(X_train)\n",
    "y_test_pred = mlr_model.predict(X_test)\n",
    "\n",
    "# TODO: Print train and test performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74dc7f",
   "metadata": {
    "id": "ed74dc7f"
   },
   "source": [
    "## Inspect Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lwa12kqppya",
   "source": "### Interpreting Model Coefficients\n\nAfter training a linear model, coefficients tell us about feature importance:\n\n**Positive coefficient:**\n- As feature increases, prediction increases\n- Example: Higher age → higher insurance charges\n\n**Negative coefficient:**\n- As feature increases, prediction decreases\n- Example: Being non-smoker → lower charges\n\n**Magnitude matters:**\n- Larger absolute value = stronger effect\n- Small coefficient = weak effect\n- Near zero = little to no effect\n\n**One-hot encoded features:**\n- Coefficient represents difference from dropped category\n- Example: region_northeast = +500 means northeast has $500 higher charges than reference\n\n**Important considerations:**\n- Correlation doesn't imply causation\n- Coefficients depend on feature scaling\n- Correlated features can have unstable coefficients\n- Always consider domain knowledge when interpreting",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075cae7d",
   "metadata": {
    "id": "075cae7d"
   },
   "outputs": [],
   "source": [
    "# Extract feature names after OneHotEncoding\n",
    "ohe = mlr_model.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "all_feature_names = numeric_features + list(cat_feature_names)\n",
    "\n",
    "# Extract model coefficients\n",
    "linreg = mlr_model.named_steps[\"linreg\"]\n",
    "coeffs = pd.DataFrame({\"feature\": all_feature_names, \"coefficient\": linreg.coef_})\n",
    "\n",
    "# Print intercept and coefficients\n",
    "print(\"Intercept:\", linreg.intercept_)\n",
    "coeffs.sort_values(\"coefficient\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44effc6",
   "metadata": {
    "id": "d44effc6"
   },
   "source": [
    "## Plot Actual vs Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jdd4c70sxjq",
   "source": "### Visualizing Predictions vs Actual\n\nA scatter plot of predicted vs actual values is one of the most important visualizations for regression models:\n\n**What the plot shows:**\n- X-axis: Actual (true) values\n- Y-axis: Predicted values\n- Each point = one prediction\n\n**Ideal pattern:**\n- Points form a diagonal line from bottom-left to top-right\n- This means predicted = actual (perfect predictions)\n\n**Common patterns and what they mean:**\n1. **Tight around diagonal**: Good model\n2. **Wide spread**: Model has high error\n3. **Above diagonal**: Systematic underprediction\n4. **Below diagonal**: Systematic overprediction\n5. **Curve**: Model missing non-linear pattern\n\n**The diagonal reference line:**\n- Shows where perfect predictions would fall\n- Helps assess model quality visually\n- Distance from line = prediction error\n\n**Why this visualization matters:**\n- Reveals patterns not captured by metrics alone\n- Shows if errors are consistent across value ranges\n- Helps identify systematic biases in predictions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb01133",
   "metadata": {
    "id": "aeb01133"
   },
   "outputs": [],
   "source": [
    "# Plot scatter of actual vs predicted\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.4)\n",
    "plt.xlabel(\"Actual charges\")\n",
    "plt.ylabel(\"Predicted charges\")\n",
    "plt.title(\"Actual vs Predicted (Test Set)\")\n",
    "\n",
    "# Add diagonal line\n",
    "lims = [min(y_test.min(), y_test_pred.min()), max(y_test.max(), y_test_pred.max())]\n",
    "plt.plot(lims, lims)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a3dce",
   "metadata": {
    "id": "789a3dce"
   },
   "source": [
    "## Part B: Polynomial Regression Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u7zcfxnqwfq",
   "source": "### Understanding Polynomial Regression on Synthetic Data\n\nThis section uses synthetic (artificially generated) data to clearly demonstrate polynomial regression concepts:\n\n**Why use synthetic data?**\n- We know the true relationship (perfect for learning)\n- Can control the complexity\n- No noise from real-world complications\n- Clear visualization of concepts\n\n**The generated data:**\n- X: Hours studied (0 to 8)\n- y: Exam scores with a curved relationship\n- Equation: score = 35 + 12*hours - 1*hours² + noise\n- This creates a parabola shape (peaks then declines)\n\n**What you'll see:**\n1. **Linear fit (degree 1)**: Straight line - underfits the curve\n2. **Quadratic fit (degree 2)**: Perfect match - captures the parabola\n3. **Cubic fit (degree 3)**: Slight overfit - extra wiggles\n4. **High degree (degree 8)**: Severe overfit - passes through every point\n\n**Key learning goals:**\n- Understand how polynomial degree affects fit\n- Visualize underfitting vs overfitting\n- See why sometimes simpler is better",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f1ae2",
   "metadata": {
    "id": "ac2f1ae2"
   },
   "outputs": [],
   "source": "# Generate synthetic curved dataset\nnp.random.seed(42)  # Set random seed for reproducible results\n\n# Create X values: 80 points evenly spaced from 0 to 8 hours\nX_hours = np.linspace(0, 8, 80).reshape(-1, 1)\n# np.linspace creates evenly spaced numbers\n# reshape(-1, 1) converts to 2D array (required by scikit-learn)\n\n# Generate y values with a quadratic relationship plus random noise\nnoise = np.random.normal(0, 5, size=X_hours.shape[0])\n# Add Gaussian noise with mean=0, std=5\n# This makes the relationship more realistic\n\n# True relationship: y = 35 + 12x - x²\ny_scores = 35 + 12 * X_hours[:, 0] - 1 * (X_hours[:, 0] ** 2) + noise\n# X_hours[:, 0] extracts the values from 2D array\n# This creates an inverted parabola (goes up then down)\n\n# Plot the data to see the pattern\nplt.figure(figsize=(10, 6))\nplt.scatter(X_hours, y_scores, alpha=0.5)\n# Scatter plot shows individual data points\n# alpha=0.5 makes points semi-transparent\n\nplt.xlabel(\"Hours studied\")\nplt.ylabel(\"Exam score\")\nplt.title(\"Exam Score (Curved Relationship)\")\nplt.grid(True)\nplt.show()\n\n# What this plot shows:\n# - Score increases with study time up to a point\n# - After ~6 hours, scores decrease (burnout?)\n# - This is a classic inverted parabola shape\n# - Noise makes it more realistic than perfect data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ca4fd",
   "metadata": {
    "id": "f91ca4fd"
   },
   "outputs": [],
   "source": "# Train test split for polynomial data\nX_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n    X_hours, y_scores, test_size=0.2, random_state=42\n)\n# Split 80% training, 20% testing\n# Same random_state ensures reproducible split\n# Training: 64 samples, Testing: 16 samples\n\nprint(f\"Training set shape: {X_train_h.shape}\")\nprint(f\"Test set shape: {X_test_h.shape}\")\n\n# Why we need train/test split here:\n# 1. To detect overfitting in high-degree polynomials\n# 2. To ensure our model generalizes to new data\n# 3. To compare performance across different degrees\n# 4. To find the optimal complexity level\n\n# With synthetic data:\n# We know degree 2 is the true relationship\n- But the model doesn't know this\n- We'll see if train/test split helps identify this\n- Higher degrees will overfit to noise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ca254",
   "metadata": {
    "id": "4b3ca254"
   },
   "outputs": [],
   "source": [
    "# TODO: Create helper to fit polynomial model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae254f8c",
   "metadata": {
    "id": "ae254f8c"
   },
   "outputs": [],
   "source": [
    "# TODO: Fit models for degrees 1, 2, 3, 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4f1d0",
   "metadata": {
    "id": "b2f4f1d0"
   },
   "outputs": [],
   "source": [
    "# TODO: Plot fitted curves for each degree (R2,RMSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf2642",
   "metadata": {
    "id": "8acf2642"
   },
   "source": "## TODO: Final Reflection\n\nAfter completing all the exercises, write down your answers to these questions:\n\n1. **Which polynomial degree overfits and why?**\n   - Look at the R² and RMSE values for different degrees\n   - Compare training vs test performance\n   - Visualize the fitted curves\n   - Explain why this degree overfits (too many parameters, fits noise)\n\n2. **Which degree gives the best generalization?**\n   - Which degree has the best test set performance?\n   - Is there a large gap between train and test scores?\n   - How does this compare to the true relationship (degree 2)?\n\n3. **What did you learn about MLR and polynomial regression?**\n   - How do MLR and polynomial regression differ?\n   - When would you use each?\n   - What are the pros and cons of polynomial regression?\n   - What's the relationship between model complexity and overfitting?\n\n4. **How would you apply these concepts to real-world problems?**\n   - When would you choose polynomial over linear?\n   - How would you select the right degree?\n   - What other techniques could help prevent overfitting?\n\n**Bonus question:**\n- How does the synthetic data example differ from the insurance dataset?\n- What additional challenges do real-world datasets present?"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1IdpLUDERn0fhhdbcszzO-QW7oOyBB6I1",
     "timestamp": 1765185516690
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}